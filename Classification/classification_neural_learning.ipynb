{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_credit.plk','rb') as f:\n",
    " x_credit_train,y_credit_train,x_credit_test,y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_test.shape, y_credit_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_train.shape, y_credit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MLPClassifier(max_iter=1500,verbose=True,tol=0.0000100,solver='adam', activation='relu', hidden_layer_sizes=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.56474702\n",
      "Iteration 2, loss = 1.54260046\n",
      "Iteration 3, loss = 1.52116288\n",
      "Iteration 4, loss = 1.50001617\n",
      "Iteration 5, loss = 1.47924978\n",
      "Iteration 6, loss = 1.45886884\n",
      "Iteration 7, loss = 1.43908502\n",
      "Iteration 8, loss = 1.41987094\n",
      "Iteration 9, loss = 1.40092365\n",
      "Iteration 10, loss = 1.38267656\n",
      "Iteration 11, loss = 1.36493519\n",
      "Iteration 12, loss = 1.34753169\n",
      "Iteration 13, loss = 1.33092221\n",
      "Iteration 14, loss = 1.31445893\n",
      "Iteration 15, loss = 1.29852114\n",
      "Iteration 16, loss = 1.28304365\n",
      "Iteration 17, loss = 1.26793186\n",
      "Iteration 18, loss = 1.25341904\n",
      "Iteration 19, loss = 1.23920922\n",
      "Iteration 20, loss = 1.22551351\n",
      "Iteration 21, loss = 1.21227612\n",
      "Iteration 22, loss = 1.19940190\n",
      "Iteration 23, loss = 1.18691052\n",
      "Iteration 24, loss = 1.17477361\n",
      "Iteration 25, loss = 1.16316109\n",
      "Iteration 26, loss = 1.15174883\n",
      "Iteration 27, loss = 1.14077503\n",
      "Iteration 28, loss = 1.13012181\n",
      "Iteration 29, loss = 1.11965214\n",
      "Iteration 30, loss = 1.10954789\n",
      "Iteration 31, loss = 1.09959063\n",
      "Iteration 32, loss = 1.08992304\n",
      "Iteration 33, loss = 1.08059937\n",
      "Iteration 34, loss = 1.07149722\n",
      "Iteration 35, loss = 1.06263029\n",
      "Iteration 36, loss = 1.05398446\n",
      "Iteration 37, loss = 1.04565474\n",
      "Iteration 38, loss = 1.03746877\n",
      "Iteration 39, loss = 1.02947453\n",
      "Iteration 40, loss = 1.02168693\n",
      "Iteration 41, loss = 1.01412870\n",
      "Iteration 42, loss = 1.00668419\n",
      "Iteration 43, loss = 0.99952498\n",
      "Iteration 44, loss = 0.99248765\n",
      "Iteration 45, loss = 0.98559113\n",
      "Iteration 46, loss = 0.97882064\n",
      "Iteration 47, loss = 0.97225042\n",
      "Iteration 48, loss = 0.96573207\n",
      "Iteration 49, loss = 0.95942005\n",
      "Iteration 50, loss = 0.95319777\n",
      "Iteration 51, loss = 0.94700781\n",
      "Iteration 52, loss = 0.94103901\n",
      "Iteration 53, loss = 0.93516859\n",
      "Iteration 54, loss = 0.92933199\n",
      "Iteration 55, loss = 0.92365324\n",
      "Iteration 56, loss = 0.91806649\n",
      "Iteration 57, loss = 0.91261543\n",
      "Iteration 58, loss = 0.90725002\n",
      "Iteration 59, loss = 0.90199193\n",
      "Iteration 60, loss = 0.89672983\n",
      "Iteration 61, loss = 0.89167836\n",
      "Iteration 62, loss = 0.88662092\n",
      "Iteration 63, loss = 0.88167522\n",
      "Iteration 64, loss = 0.87686078\n",
      "Iteration 65, loss = 0.87213881\n",
      "Iteration 66, loss = 0.86743465\n",
      "Iteration 67, loss = 0.86285360\n",
      "Iteration 68, loss = 0.85839845\n",
      "Iteration 69, loss = 0.85391305\n",
      "Iteration 70, loss = 0.84957373\n",
      "Iteration 71, loss = 0.84536938\n",
      "Iteration 72, loss = 0.84112349\n",
      "Iteration 73, loss = 0.83700139\n",
      "Iteration 74, loss = 0.83293682\n",
      "Iteration 75, loss = 0.82892296\n",
      "Iteration 76, loss = 0.82494570\n",
      "Iteration 77, loss = 0.82103732\n",
      "Iteration 78, loss = 0.81715491\n",
      "Iteration 79, loss = 0.81334816\n",
      "Iteration 80, loss = 0.80953696\n",
      "Iteration 81, loss = 0.80579826\n",
      "Iteration 82, loss = 0.80211816\n",
      "Iteration 83, loss = 0.79845223\n",
      "Iteration 84, loss = 0.79486723\n",
      "Iteration 85, loss = 0.79131970\n",
      "Iteration 86, loss = 0.78783854\n",
      "Iteration 87, loss = 0.78438595\n",
      "Iteration 88, loss = 0.78097373\n",
      "Iteration 89, loss = 0.77760159\n",
      "Iteration 90, loss = 0.77428033\n",
      "Iteration 91, loss = 0.77096718\n",
      "Iteration 92, loss = 0.76770522\n",
      "Iteration 93, loss = 0.76455339\n",
      "Iteration 94, loss = 0.76132232\n",
      "Iteration 95, loss = 0.75820521\n",
      "Iteration 96, loss = 0.75511187\n",
      "Iteration 97, loss = 0.75203768\n",
      "Iteration 98, loss = 0.74900516\n",
      "Iteration 99, loss = 0.74602783\n",
      "Iteration 100, loss = 0.74303881\n",
      "Iteration 101, loss = 0.74013946\n",
      "Iteration 102, loss = 0.73720686\n",
      "Iteration 103, loss = 0.73434061\n",
      "Iteration 104, loss = 0.73152767\n",
      "Iteration 105, loss = 0.72865150\n",
      "Iteration 106, loss = 0.72587654\n",
      "Iteration 107, loss = 0.72316493\n",
      "Iteration 108, loss = 0.72040213\n",
      "Iteration 109, loss = 0.71769921\n",
      "Iteration 110, loss = 0.71500154\n",
      "Iteration 111, loss = 0.71233130\n",
      "Iteration 112, loss = 0.70971627\n",
      "Iteration 113, loss = 0.70711839\n",
      "Iteration 114, loss = 0.70455951\n",
      "Iteration 115, loss = 0.70197873\n",
      "Iteration 116, loss = 0.69946082\n",
      "Iteration 117, loss = 0.69700127\n",
      "Iteration 118, loss = 0.69450250\n",
      "Iteration 119, loss = 0.69203730\n",
      "Iteration 120, loss = 0.68964419\n",
      "Iteration 121, loss = 0.68724877\n",
      "Iteration 122, loss = 0.68488475\n",
      "Iteration 123, loss = 0.68249686\n",
      "Iteration 124, loss = 0.68019585\n",
      "Iteration 125, loss = 0.67791426\n",
      "Iteration 126, loss = 0.67562171\n",
      "Iteration 127, loss = 0.67335113\n",
      "Iteration 128, loss = 0.67111400\n",
      "Iteration 129, loss = 0.66888746\n",
      "Iteration 130, loss = 0.66665051\n",
      "Iteration 131, loss = 0.66448036\n",
      "Iteration 132, loss = 0.66229777\n",
      "Iteration 133, loss = 0.66014701\n",
      "Iteration 134, loss = 0.65801515\n",
      "Iteration 135, loss = 0.65589262\n",
      "Iteration 136, loss = 0.65380183\n",
      "Iteration 137, loss = 0.65174205\n",
      "Iteration 138, loss = 0.64965836\n",
      "Iteration 139, loss = 0.64762555\n",
      "Iteration 140, loss = 0.64562181\n",
      "Iteration 141, loss = 0.64363466\n",
      "Iteration 142, loss = 0.64161745\n",
      "Iteration 143, loss = 0.63964102\n",
      "Iteration 144, loss = 0.63769254\n",
      "Iteration 145, loss = 0.63573668\n",
      "Iteration 146, loss = 0.63380295\n",
      "Iteration 147, loss = 0.63190492\n",
      "Iteration 148, loss = 0.62998697\n",
      "Iteration 149, loss = 0.62811922\n",
      "Iteration 150, loss = 0.62624506\n",
      "Iteration 151, loss = 0.62440353\n",
      "Iteration 152, loss = 0.62256135\n",
      "Iteration 153, loss = 0.62077678\n",
      "Iteration 154, loss = 0.61893842\n",
      "Iteration 155, loss = 0.61713807\n",
      "Iteration 156, loss = 0.61536931\n",
      "Iteration 157, loss = 0.61356557\n",
      "Iteration 158, loss = 0.61183652\n",
      "Iteration 159, loss = 0.61007835\n",
      "Iteration 160, loss = 0.60834041\n",
      "Iteration 161, loss = 0.60663790\n",
      "Iteration 162, loss = 0.60492768\n",
      "Iteration 163, loss = 0.60322191\n",
      "Iteration 164, loss = 0.60157547\n",
      "Iteration 165, loss = 0.59988777\n",
      "Iteration 166, loss = 0.59823152\n",
      "Iteration 167, loss = 0.59659411\n",
      "Iteration 168, loss = 0.59495378\n",
      "Iteration 169, loss = 0.59334668\n",
      "Iteration 170, loss = 0.59176244\n",
      "Iteration 171, loss = 0.59014356\n",
      "Iteration 172, loss = 0.58857372\n",
      "Iteration 173, loss = 0.58701124\n",
      "Iteration 174, loss = 0.58544799\n",
      "Iteration 175, loss = 0.58391017\n",
      "Iteration 176, loss = 0.58240974\n",
      "Iteration 177, loss = 0.58087011\n",
      "Iteration 178, loss = 0.57938834\n",
      "Iteration 179, loss = 0.57790506\n",
      "Iteration 180, loss = 0.57646239\n",
      "Iteration 181, loss = 0.57501597\n",
      "Iteration 182, loss = 0.57359565\n",
      "Iteration 183, loss = 0.57219711\n",
      "Iteration 184, loss = 0.57080932\n",
      "Iteration 185, loss = 0.56942092\n",
      "Iteration 186, loss = 0.56803235\n",
      "Iteration 187, loss = 0.56667676\n",
      "Iteration 188, loss = 0.56532348\n",
      "Iteration 189, loss = 0.56398364\n",
      "Iteration 190, loss = 0.56264264\n",
      "Iteration 191, loss = 0.56134144\n",
      "Iteration 192, loss = 0.56000990\n",
      "Iteration 193, loss = 0.55871795\n",
      "Iteration 194, loss = 0.55741912\n",
      "Iteration 195, loss = 0.55611906\n",
      "Iteration 196, loss = 0.55485467\n",
      "Iteration 197, loss = 0.55361452\n",
      "Iteration 198, loss = 0.55234283\n",
      "Iteration 199, loss = 0.55110089\n",
      "Iteration 200, loss = 0.54990037\n",
      "Iteration 201, loss = 0.54866548\n",
      "Iteration 202, loss = 0.54746747\n",
      "Iteration 203, loss = 0.54625816\n",
      "Iteration 204, loss = 0.54507034\n",
      "Iteration 205, loss = 0.54388858\n",
      "Iteration 206, loss = 0.54271097\n",
      "Iteration 207, loss = 0.54153878\n",
      "Iteration 208, loss = 0.54038836\n",
      "Iteration 209, loss = 0.53923334\n",
      "Iteration 210, loss = 0.53808293\n",
      "Iteration 211, loss = 0.53694300\n",
      "Iteration 212, loss = 0.53582918\n",
      "Iteration 213, loss = 0.53471925\n",
      "Iteration 214, loss = 0.53359539\n",
      "Iteration 215, loss = 0.53248069\n",
      "Iteration 216, loss = 0.53142323\n",
      "Iteration 217, loss = 0.53033468\n",
      "Iteration 218, loss = 0.52927022\n",
      "Iteration 219, loss = 0.52818850\n",
      "Iteration 220, loss = 0.52714696\n",
      "Iteration 221, loss = 0.52611153\n",
      "Iteration 222, loss = 0.52506007\n",
      "Iteration 223, loss = 0.52401418\n",
      "Iteration 224, loss = 0.52301553\n",
      "Iteration 225, loss = 0.52197543\n",
      "Iteration 226, loss = 0.52098364\n",
      "Iteration 227, loss = 0.51996443\n",
      "Iteration 228, loss = 0.51898350\n",
      "Iteration 229, loss = 0.51800128\n",
      "Iteration 230, loss = 0.51701198\n",
      "Iteration 231, loss = 0.51602954\n",
      "Iteration 232, loss = 0.51506521\n",
      "Iteration 233, loss = 0.51407531\n",
      "Iteration 234, loss = 0.51314744\n",
      "Iteration 235, loss = 0.51220115\n",
      "Iteration 236, loss = 0.51128035\n",
      "Iteration 237, loss = 0.51031594\n",
      "Iteration 238, loss = 0.50941909\n",
      "Iteration 239, loss = 0.50849151\n",
      "Iteration 240, loss = 0.50756709\n",
      "Iteration 241, loss = 0.50666975\n",
      "Iteration 242, loss = 0.50575802\n",
      "Iteration 243, loss = 0.50488435\n",
      "Iteration 244, loss = 0.50399646\n",
      "Iteration 245, loss = 0.50312191\n",
      "Iteration 246, loss = 0.50224790\n",
      "Iteration 247, loss = 0.50140302\n",
      "Iteration 248, loss = 0.50055226\n",
      "Iteration 249, loss = 0.49969523\n",
      "Iteration 250, loss = 0.49885819\n",
      "Iteration 251, loss = 0.49801786\n",
      "Iteration 252, loss = 0.49718998\n",
      "Iteration 253, loss = 0.49634934\n",
      "Iteration 254, loss = 0.49552260\n",
      "Iteration 255, loss = 0.49470610\n",
      "Iteration 256, loss = 0.49390787\n",
      "Iteration 257, loss = 0.49309125\n",
      "Iteration 258, loss = 0.49227678\n",
      "Iteration 259, loss = 0.49151718\n",
      "Iteration 260, loss = 0.49074495\n",
      "Iteration 261, loss = 0.48994707\n",
      "Iteration 262, loss = 0.48920093\n",
      "Iteration 263, loss = 0.48843244\n",
      "Iteration 264, loss = 0.48765580\n",
      "Iteration 265, loss = 0.48692287\n",
      "Iteration 266, loss = 0.48617240\n",
      "Iteration 267, loss = 0.48543986\n",
      "Iteration 268, loss = 0.48472491\n",
      "Iteration 269, loss = 0.48398106\n",
      "Iteration 270, loss = 0.48325538\n",
      "Iteration 271, loss = 0.48256110\n",
      "Iteration 272, loss = 0.48181392\n",
      "Iteration 273, loss = 0.48111444\n",
      "Iteration 274, loss = 0.48040681\n",
      "Iteration 275, loss = 0.47971898\n",
      "Iteration 276, loss = 0.47902184\n",
      "Iteration 277, loss = 0.47831895\n",
      "Iteration 278, loss = 0.47765181\n",
      "Iteration 279, loss = 0.47697822\n",
      "Iteration 280, loss = 0.47629914\n",
      "Iteration 281, loss = 0.47562949\n",
      "Iteration 282, loss = 0.47497537\n",
      "Iteration 283, loss = 0.47428742\n",
      "Iteration 284, loss = 0.47367508\n",
      "Iteration 285, loss = 0.47302470\n",
      "Iteration 286, loss = 0.47239178\n",
      "Iteration 287, loss = 0.47173189\n",
      "Iteration 288, loss = 0.47111601\n",
      "Iteration 289, loss = 0.47050159\n",
      "Iteration 290, loss = 0.46985961\n",
      "Iteration 291, loss = 0.46924926\n",
      "Iteration 292, loss = 0.46863690\n",
      "Iteration 293, loss = 0.46802039\n",
      "Iteration 294, loss = 0.46740904\n",
      "Iteration 295, loss = 0.46681711\n",
      "Iteration 296, loss = 0.46621170\n",
      "Iteration 297, loss = 0.46560589\n",
      "Iteration 298, loss = 0.46503148\n",
      "Iteration 299, loss = 0.46443197\n",
      "Iteration 300, loss = 0.46386795\n",
      "Iteration 301, loss = 0.46328106\n",
      "Iteration 302, loss = 0.46271113\n",
      "Iteration 303, loss = 0.46214334\n",
      "Iteration 304, loss = 0.46157692\n",
      "Iteration 305, loss = 0.46101111\n",
      "Iteration 306, loss = 0.46046481\n",
      "Iteration 307, loss = 0.45990213\n",
      "Iteration 308, loss = 0.45937383\n",
      "Iteration 309, loss = 0.45882291\n",
      "Iteration 310, loss = 0.45827750\n",
      "Iteration 311, loss = 0.45774049\n",
      "Iteration 312, loss = 0.45719612\n",
      "Iteration 313, loss = 0.45667205\n",
      "Iteration 314, loss = 0.45613385\n",
      "Iteration 315, loss = 0.45562307\n",
      "Iteration 316, loss = 0.45509581\n",
      "Iteration 317, loss = 0.45456224\n",
      "Iteration 318, loss = 0.45406450\n",
      "Iteration 319, loss = 0.45354686\n",
      "Iteration 320, loss = 0.45304108\n",
      "Iteration 321, loss = 0.45253587\n",
      "Iteration 322, loss = 0.45203551\n",
      "Iteration 323, loss = 0.45154986\n",
      "Iteration 324, loss = 0.45105087\n",
      "Iteration 325, loss = 0.45056057\n",
      "Iteration 326, loss = 0.45008418\n",
      "Iteration 327, loss = 0.44960773\n",
      "Iteration 328, loss = 0.44912540\n",
      "Iteration 329, loss = 0.44864098\n",
      "Iteration 330, loss = 0.44817158\n",
      "Iteration 331, loss = 0.44769780\n",
      "Iteration 332, loss = 0.44722059\n",
      "Iteration 333, loss = 0.44674879\n",
      "Iteration 334, loss = 0.44628159\n",
      "Iteration 335, loss = 0.44582143\n",
      "Iteration 336, loss = 0.44537832\n",
      "Iteration 337, loss = 0.44489707\n",
      "Iteration 338, loss = 0.44444356\n",
      "Iteration 339, loss = 0.44398614\n",
      "Iteration 340, loss = 0.44353902\n",
      "Iteration 341, loss = 0.44308667\n",
      "Iteration 342, loss = 0.44264874\n",
      "Iteration 343, loss = 0.44218485\n",
      "Iteration 344, loss = 0.44173901\n",
      "Iteration 345, loss = 0.44131128\n",
      "Iteration 346, loss = 0.44085553\n",
      "Iteration 347, loss = 0.44040512\n",
      "Iteration 348, loss = 0.43997155\n",
      "Iteration 349, loss = 0.43954111\n",
      "Iteration 350, loss = 0.43910514\n",
      "Iteration 351, loss = 0.43867123\n",
      "Iteration 352, loss = 0.43825155\n",
      "Iteration 353, loss = 0.43781545\n",
      "Iteration 354, loss = 0.43738788\n",
      "Iteration 355, loss = 0.43696247\n",
      "Iteration 356, loss = 0.43652836\n",
      "Iteration 357, loss = 0.43610948\n",
      "Iteration 358, loss = 0.43568761\n",
      "Iteration 359, loss = 0.43525570\n",
      "Iteration 360, loss = 0.43483939\n",
      "Iteration 361, loss = 0.43442327\n",
      "Iteration 362, loss = 0.43400316\n",
      "Iteration 363, loss = 0.43358179\n",
      "Iteration 364, loss = 0.43317245\n",
      "Iteration 365, loss = 0.43274867\n",
      "Iteration 366, loss = 0.43232850\n",
      "Iteration 367, loss = 0.43192560\n",
      "Iteration 368, loss = 0.43150447\n",
      "Iteration 369, loss = 0.43109697\n",
      "Iteration 370, loss = 0.43067088\n",
      "Iteration 371, loss = 0.43026726\n",
      "Iteration 372, loss = 0.42984039\n",
      "Iteration 373, loss = 0.42942810\n",
      "Iteration 374, loss = 0.42902183\n",
      "Iteration 375, loss = 0.42860150\n",
      "Iteration 376, loss = 0.42818676\n",
      "Iteration 377, loss = 0.42778071\n",
      "Iteration 378, loss = 0.42735687\n",
      "Iteration 379, loss = 0.42694557\n",
      "Iteration 380, loss = 0.42652760\n",
      "Iteration 381, loss = 0.42610918\n",
      "Iteration 382, loss = 0.42568372\n",
      "Iteration 383, loss = 0.42526637\n",
      "Iteration 384, loss = 0.42483595\n",
      "Iteration 385, loss = 0.42440864\n",
      "Iteration 386, loss = 0.42398506\n",
      "Iteration 387, loss = 0.42354894\n",
      "Iteration 388, loss = 0.42312125\n",
      "Iteration 389, loss = 0.42269154\n",
      "Iteration 390, loss = 0.42224659\n",
      "Iteration 391, loss = 0.42180741\n",
      "Iteration 392, loss = 0.42137157\n",
      "Iteration 393, loss = 0.42092986\n",
      "Iteration 394, loss = 0.42048625\n",
      "Iteration 395, loss = 0.42003843\n",
      "Iteration 396, loss = 0.41959334\n",
      "Iteration 397, loss = 0.41914496\n",
      "Iteration 398, loss = 0.41869339\n",
      "Iteration 399, loss = 0.41823821\n",
      "Iteration 400, loss = 0.41778040\n",
      "Iteration 401, loss = 0.41732594\n",
      "Iteration 402, loss = 0.41685996\n",
      "Iteration 403, loss = 0.41638867\n",
      "Iteration 404, loss = 0.41592040\n",
      "Iteration 405, loss = 0.41544748\n",
      "Iteration 406, loss = 0.41497580\n",
      "Iteration 407, loss = 0.41449806\n",
      "Iteration 408, loss = 0.41402216\n",
      "Iteration 409, loss = 0.41353852\n",
      "Iteration 410, loss = 0.41305334\n",
      "Iteration 411, loss = 0.41256504\n",
      "Iteration 412, loss = 0.41206728\n",
      "Iteration 413, loss = 0.41157452\n",
      "Iteration 414, loss = 0.41107027\n",
      "Iteration 415, loss = 0.41056475\n",
      "Iteration 416, loss = 0.41004837\n",
      "Iteration 417, loss = 0.40953385\n",
      "Iteration 418, loss = 0.40900303\n",
      "Iteration 419, loss = 0.40846881\n",
      "Iteration 420, loss = 0.40793563\n",
      "Iteration 421, loss = 0.40739250\n",
      "Iteration 422, loss = 0.40685696\n",
      "Iteration 423, loss = 0.40630049\n",
      "Iteration 424, loss = 0.40575575\n",
      "Iteration 425, loss = 0.40518924\n",
      "Iteration 426, loss = 0.40463613\n",
      "Iteration 427, loss = 0.40406353\n",
      "Iteration 428, loss = 0.40349512\n",
      "Iteration 429, loss = 0.40291540\n",
      "Iteration 430, loss = 0.40232913\n",
      "Iteration 431, loss = 0.40174299\n",
      "Iteration 432, loss = 0.40114689\n",
      "Iteration 433, loss = 0.40055143\n",
      "Iteration 434, loss = 0.39994509\n",
      "Iteration 435, loss = 0.39932893\n",
      "Iteration 436, loss = 0.39870303\n",
      "Iteration 437, loss = 0.39808026\n",
      "Iteration 438, loss = 0.39741495\n",
      "Iteration 439, loss = 0.39676225\n",
      "Iteration 440, loss = 0.39609972\n",
      "Iteration 441, loss = 0.39541928\n",
      "Iteration 442, loss = 0.39475133\n",
      "Iteration 443, loss = 0.39405023\n",
      "Iteration 444, loss = 0.39336422\n",
      "Iteration 445, loss = 0.39265671\n",
      "Iteration 446, loss = 0.39195432\n",
      "Iteration 447, loss = 0.39121714\n",
      "Iteration 448, loss = 0.39049277\n",
      "Iteration 449, loss = 0.38975248\n",
      "Iteration 450, loss = 0.38899763\n",
      "Iteration 451, loss = 0.38823991\n",
      "Iteration 452, loss = 0.38744868\n",
      "Iteration 453, loss = 0.38667152\n",
      "Iteration 454, loss = 0.38586365\n",
      "Iteration 455, loss = 0.38505361\n",
      "Iteration 456, loss = 0.38423573\n",
      "Iteration 457, loss = 0.38336668\n",
      "Iteration 458, loss = 0.38253571\n",
      "Iteration 459, loss = 0.38166911\n",
      "Iteration 460, loss = 0.38080320\n",
      "Iteration 461, loss = 0.37992040\n",
      "Iteration 462, loss = 0.37905586\n",
      "Iteration 463, loss = 0.37815359\n",
      "Iteration 464, loss = 0.37724532\n",
      "Iteration 465, loss = 0.37633040\n",
      "Iteration 466, loss = 0.37543841\n",
      "Iteration 467, loss = 0.37446439\n",
      "Iteration 468, loss = 0.37354552\n",
      "Iteration 469, loss = 0.37259657\n",
      "Iteration 470, loss = 0.37163855\n",
      "Iteration 471, loss = 0.37067851\n",
      "Iteration 472, loss = 0.36969463\n",
      "Iteration 473, loss = 0.36868658\n",
      "Iteration 474, loss = 0.36772471\n",
      "Iteration 475, loss = 0.36669451\n",
      "Iteration 476, loss = 0.36567745\n",
      "Iteration 477, loss = 0.36466578\n",
      "Iteration 478, loss = 0.36360296\n",
      "Iteration 479, loss = 0.36254702\n",
      "Iteration 480, loss = 0.36149425\n",
      "Iteration 481, loss = 0.36043613\n",
      "Iteration 482, loss = 0.35934302\n",
      "Iteration 483, loss = 0.35825241\n",
      "Iteration 484, loss = 0.35714293\n",
      "Iteration 485, loss = 0.35601739\n",
      "Iteration 486, loss = 0.35489213\n",
      "Iteration 487, loss = 0.35372745\n",
      "Iteration 488, loss = 0.35256075\n",
      "Iteration 489, loss = 0.35138283\n",
      "Iteration 490, loss = 0.35019911\n",
      "Iteration 491, loss = 0.34900156\n",
      "Iteration 492, loss = 0.34779122\n",
      "Iteration 493, loss = 0.34656052\n",
      "Iteration 494, loss = 0.34532921\n",
      "Iteration 495, loss = 0.34404328\n",
      "Iteration 496, loss = 0.34282246\n",
      "Iteration 497, loss = 0.34154920\n",
      "Iteration 498, loss = 0.34027092\n",
      "Iteration 499, loss = 0.33902989\n",
      "Iteration 500, loss = 0.33764937\n",
      "Iteration 501, loss = 0.33635780\n",
      "Iteration 502, loss = 0.33503340\n",
      "Iteration 503, loss = 0.33370047\n",
      "Iteration 504, loss = 0.33231471\n",
      "Iteration 505, loss = 0.33097080\n",
      "Iteration 506, loss = 0.32952202\n",
      "Iteration 507, loss = 0.32809770\n",
      "Iteration 508, loss = 0.32657943\n",
      "Iteration 509, loss = 0.32514665\n",
      "Iteration 510, loss = 0.32368130\n",
      "Iteration 511, loss = 0.32219563\n",
      "Iteration 512, loss = 0.32071082\n",
      "Iteration 513, loss = 0.31926778\n",
      "Iteration 514, loss = 0.31771241\n",
      "Iteration 515, loss = 0.31612670\n",
      "Iteration 516, loss = 0.31458111\n",
      "Iteration 517, loss = 0.31302737\n",
      "Iteration 518, loss = 0.31135418\n",
      "Iteration 519, loss = 0.30972879\n",
      "Iteration 520, loss = 0.30807459\n",
      "Iteration 521, loss = 0.30638681\n",
      "Iteration 522, loss = 0.30466378\n",
      "Iteration 523, loss = 0.30294337\n",
      "Iteration 524, loss = 0.30114428\n",
      "Iteration 525, loss = 0.29940093\n",
      "Iteration 526, loss = 0.29761135\n",
      "Iteration 527, loss = 0.29578463\n",
      "Iteration 528, loss = 0.29403947\n",
      "Iteration 529, loss = 0.29234759\n",
      "Iteration 530, loss = 0.29061212\n",
      "Iteration 531, loss = 0.28887066\n",
      "Iteration 532, loss = 0.28715657\n",
      "Iteration 533, loss = 0.28540839\n",
      "Iteration 534, loss = 0.28380391\n",
      "Iteration 535, loss = 0.28213080\n",
      "Iteration 536, loss = 0.28058489\n",
      "Iteration 537, loss = 0.27914827\n",
      "Iteration 538, loss = 0.27756973\n",
      "Iteration 539, loss = 0.27605049\n",
      "Iteration 540, loss = 0.27463064\n",
      "Iteration 541, loss = 0.27317075\n",
      "Iteration 542, loss = 0.27170135\n",
      "Iteration 543, loss = 0.27016364\n",
      "Iteration 544, loss = 0.26867817\n",
      "Iteration 545, loss = 0.26723928\n",
      "Iteration 546, loss = 0.26579575\n",
      "Iteration 547, loss = 0.26442911\n",
      "Iteration 548, loss = 0.26316548\n",
      "Iteration 549, loss = 0.26183894\n",
      "Iteration 550, loss = 0.26064779\n",
      "Iteration 551, loss = 0.25930307\n",
      "Iteration 552, loss = 0.25808958\n",
      "Iteration 553, loss = 0.25688035\n",
      "Iteration 554, loss = 0.25572197\n",
      "Iteration 555, loss = 0.25449604\n",
      "Iteration 556, loss = 0.25329412\n",
      "Iteration 557, loss = 0.25212664\n",
      "Iteration 558, loss = 0.25093339\n",
      "Iteration 559, loss = 0.24988188\n",
      "Iteration 560, loss = 0.24861985\n",
      "Iteration 561, loss = 0.24752578\n",
      "Iteration 562, loss = 0.24637285\n",
      "Iteration 563, loss = 0.24523498\n",
      "Iteration 564, loss = 0.24415251\n",
      "Iteration 565, loss = 0.24301876\n",
      "Iteration 566, loss = 0.24193346\n",
      "Iteration 567, loss = 0.24079575\n",
      "Iteration 568, loss = 0.23971888\n",
      "Iteration 569, loss = 0.23860742\n",
      "Iteration 570, loss = 0.23753258\n",
      "Iteration 571, loss = 0.23648687\n",
      "Iteration 572, loss = 0.23545062\n",
      "Iteration 573, loss = 0.23440636\n",
      "Iteration 574, loss = 0.23335303\n",
      "Iteration 575, loss = 0.23233472\n",
      "Iteration 576, loss = 0.23136000\n",
      "Iteration 577, loss = 0.23032367\n",
      "Iteration 578, loss = 0.22936020\n",
      "Iteration 579, loss = 0.22838668\n",
      "Iteration 580, loss = 0.22744261\n",
      "Iteration 581, loss = 0.22647872\n",
      "Iteration 582, loss = 0.22552716\n",
      "Iteration 583, loss = 0.22461553\n",
      "Iteration 584, loss = 0.22367361\n",
      "Iteration 585, loss = 0.22278955\n",
      "Iteration 586, loss = 0.22187467\n",
      "Iteration 587, loss = 0.22092896\n",
      "Iteration 588, loss = 0.22002263\n",
      "Iteration 589, loss = 0.21919283\n",
      "Iteration 590, loss = 0.21831980\n",
      "Iteration 591, loss = 0.21739281\n",
      "Iteration 592, loss = 0.21654726\n",
      "Iteration 593, loss = 0.21575258\n",
      "Iteration 594, loss = 0.21488859\n",
      "Iteration 595, loss = 0.21403757\n",
      "Iteration 596, loss = 0.21321829\n",
      "Iteration 597, loss = 0.21242092\n",
      "Iteration 598, loss = 0.21161186\n",
      "Iteration 599, loss = 0.21080076\n",
      "Iteration 600, loss = 0.21003291\n",
      "Iteration 601, loss = 0.20923393\n",
      "Iteration 602, loss = 0.20848538\n",
      "Iteration 603, loss = 0.20770352\n",
      "Iteration 604, loss = 0.20696201\n",
      "Iteration 605, loss = 0.20618387\n",
      "Iteration 606, loss = 0.20552683\n",
      "Iteration 607, loss = 0.20471016\n",
      "Iteration 608, loss = 0.20397036\n",
      "Iteration 609, loss = 0.20321746\n",
      "Iteration 610, loss = 0.20248748\n",
      "Iteration 611, loss = 0.20174857\n",
      "Iteration 612, loss = 0.20100733\n",
      "Iteration 613, loss = 0.20031120\n",
      "Iteration 614, loss = 0.19963509\n",
      "Iteration 615, loss = 0.19894523\n",
      "Iteration 616, loss = 0.19826825\n",
      "Iteration 617, loss = 0.19757252\n",
      "Iteration 618, loss = 0.19677314\n",
      "Iteration 619, loss = 0.19614348\n",
      "Iteration 620, loss = 0.19545242\n",
      "Iteration 621, loss = 0.19477758\n",
      "Iteration 622, loss = 0.19409090\n",
      "Iteration 623, loss = 0.19347809\n",
      "Iteration 624, loss = 0.19279436\n",
      "Iteration 625, loss = 0.19215288\n",
      "Iteration 626, loss = 0.19150771\n",
      "Iteration 627, loss = 0.19087151\n",
      "Iteration 628, loss = 0.19023332\n",
      "Iteration 629, loss = 0.18959207\n",
      "Iteration 630, loss = 0.18898030\n",
      "Iteration 631, loss = 0.18836358\n",
      "Iteration 632, loss = 0.18774425\n",
      "Iteration 633, loss = 0.18710404\n",
      "Iteration 634, loss = 0.18656073\n",
      "Iteration 635, loss = 0.18586999\n",
      "Iteration 636, loss = 0.18531625\n",
      "Iteration 637, loss = 0.18466650\n",
      "Iteration 638, loss = 0.18408523\n",
      "Iteration 639, loss = 0.18351574\n",
      "Iteration 640, loss = 0.18294488\n",
      "Iteration 641, loss = 0.18247643\n",
      "Iteration 642, loss = 0.18176657\n",
      "Iteration 643, loss = 0.18112456\n",
      "Iteration 644, loss = 0.18053075\n",
      "Iteration 645, loss = 0.17995410\n",
      "Iteration 646, loss = 0.17937394\n",
      "Iteration 647, loss = 0.17880844\n",
      "Iteration 648, loss = 0.17817498\n",
      "Iteration 649, loss = 0.17756075\n",
      "Iteration 650, loss = 0.17698265\n",
      "Iteration 651, loss = 0.17634928\n",
      "Iteration 652, loss = 0.17576228\n",
      "Iteration 653, loss = 0.17515691\n",
      "Iteration 654, loss = 0.17456537\n",
      "Iteration 655, loss = 0.17395743\n",
      "Iteration 656, loss = 0.17328011\n",
      "Iteration 657, loss = 0.17270854\n",
      "Iteration 658, loss = 0.17201267\n",
      "Iteration 659, loss = 0.17141657\n",
      "Iteration 660, loss = 0.17067888\n",
      "Iteration 661, loss = 0.17005007\n",
      "Iteration 662, loss = 0.16937432\n",
      "Iteration 663, loss = 0.16876473\n",
      "Iteration 664, loss = 0.16812653\n",
      "Iteration 665, loss = 0.16758027\n",
      "Iteration 666, loss = 0.16694569\n",
      "Iteration 667, loss = 0.16634204\n",
      "Iteration 668, loss = 0.16567097\n",
      "Iteration 669, loss = 0.16500023\n",
      "Iteration 670, loss = 0.16431869\n",
      "Iteration 671, loss = 0.16356846\n",
      "Iteration 672, loss = 0.16283176\n",
      "Iteration 673, loss = 0.16209782\n",
      "Iteration 674, loss = 0.16132533\n",
      "Iteration 675, loss = 0.16067907\n",
      "Iteration 676, loss = 0.15989751\n",
      "Iteration 677, loss = 0.15915403\n",
      "Iteration 678, loss = 0.15838841\n",
      "Iteration 679, loss = 0.15769284\n",
      "Iteration 680, loss = 0.15697621\n",
      "Iteration 681, loss = 0.15632275\n",
      "Iteration 682, loss = 0.15561898\n",
      "Iteration 683, loss = 0.15495709\n",
      "Iteration 684, loss = 0.15423421\n",
      "Iteration 685, loss = 0.15351256\n",
      "Iteration 686, loss = 0.15275863\n",
      "Iteration 687, loss = 0.15195893\n",
      "Iteration 688, loss = 0.15115552\n",
      "Iteration 689, loss = 0.15024500\n",
      "Iteration 690, loss = 0.14946178\n",
      "Iteration 691, loss = 0.14863409\n",
      "Iteration 692, loss = 0.14771876\n",
      "Iteration 693, loss = 0.14693672\n",
      "Iteration 694, loss = 0.14615387\n",
      "Iteration 695, loss = 0.14536029\n",
      "Iteration 696, loss = 0.14465181\n",
      "Iteration 697, loss = 0.14391176\n",
      "Iteration 698, loss = 0.14313200\n",
      "Iteration 699, loss = 0.14239192\n",
      "Iteration 700, loss = 0.14159459\n",
      "Iteration 701, loss = 0.14083251\n",
      "Iteration 702, loss = 0.14010033\n",
      "Iteration 703, loss = 0.13925677\n",
      "Iteration 704, loss = 0.13848874\n",
      "Iteration 705, loss = 0.13771476\n",
      "Iteration 706, loss = 0.13699905\n",
      "Iteration 707, loss = 0.13629181\n",
      "Iteration 708, loss = 0.13552829\n",
      "Iteration 709, loss = 0.13482845\n",
      "Iteration 710, loss = 0.13409046\n",
      "Iteration 711, loss = 0.13336033\n",
      "Iteration 712, loss = 0.13264829\n",
      "Iteration 713, loss = 0.13192187\n",
      "Iteration 714, loss = 0.13120076\n",
      "Iteration 715, loss = 0.13045019\n",
      "Iteration 716, loss = 0.12967263\n",
      "Iteration 717, loss = 0.12892155\n",
      "Iteration 718, loss = 0.12814256\n",
      "Iteration 719, loss = 0.12732780\n",
      "Iteration 720, loss = 0.12660713\n",
      "Iteration 721, loss = 0.12592286\n",
      "Iteration 722, loss = 0.12514266\n",
      "Iteration 723, loss = 0.12434690\n",
      "Iteration 724, loss = 0.12368590\n",
      "Iteration 725, loss = 0.12289446\n",
      "Iteration 726, loss = 0.12217689\n",
      "Iteration 727, loss = 0.12147359\n",
      "Iteration 728, loss = 0.12072538\n",
      "Iteration 729, loss = 0.12003063\n",
      "Iteration 730, loss = 0.11936298\n",
      "Iteration 731, loss = 0.11863110\n",
      "Iteration 732, loss = 0.11795789\n",
      "Iteration 733, loss = 0.11730232\n",
      "Iteration 734, loss = 0.11664433\n",
      "Iteration 735, loss = 0.11601308\n",
      "Iteration 736, loss = 0.11535988\n",
      "Iteration 737, loss = 0.11467020\n",
      "Iteration 738, loss = 0.11406790\n",
      "Iteration 739, loss = 0.11340556\n",
      "Iteration 740, loss = 0.11281041\n",
      "Iteration 741, loss = 0.11215422\n",
      "Iteration 742, loss = 0.11152588\n",
      "Iteration 743, loss = 0.11089168\n",
      "Iteration 744, loss = 0.11032893\n",
      "Iteration 745, loss = 0.10971956\n",
      "Iteration 746, loss = 0.10915123\n",
      "Iteration 747, loss = 0.10858721\n",
      "Iteration 748, loss = 0.10798931\n",
      "Iteration 749, loss = 0.10738989\n",
      "Iteration 750, loss = 0.10680918\n",
      "Iteration 751, loss = 0.10622049\n",
      "Iteration 752, loss = 0.10566734\n",
      "Iteration 753, loss = 0.10510711\n",
      "Iteration 754, loss = 0.10456674\n",
      "Iteration 755, loss = 0.10404103\n",
      "Iteration 756, loss = 0.10347486\n",
      "Iteration 757, loss = 0.10293653\n",
      "Iteration 758, loss = 0.10238871\n",
      "Iteration 759, loss = 0.10189377\n",
      "Iteration 760, loss = 0.10136533\n",
      "Iteration 761, loss = 0.10088106\n",
      "Iteration 762, loss = 0.10035364\n",
      "Iteration 763, loss = 0.09985009\n",
      "Iteration 764, loss = 0.09936967\n",
      "Iteration 765, loss = 0.09889687\n",
      "Iteration 766, loss = 0.09838247\n",
      "Iteration 767, loss = 0.09792123\n",
      "Iteration 768, loss = 0.09746569\n",
      "Iteration 769, loss = 0.09698824\n",
      "Iteration 770, loss = 0.09652421\n",
      "Iteration 771, loss = 0.09605021\n",
      "Iteration 772, loss = 0.09563788\n",
      "Iteration 773, loss = 0.09514928\n",
      "Iteration 774, loss = 0.09471507\n",
      "Iteration 775, loss = 0.09430742\n",
      "Iteration 776, loss = 0.09378065\n",
      "Iteration 777, loss = 0.09332214\n",
      "Iteration 778, loss = 0.09293139\n",
      "Iteration 779, loss = 0.09247523\n",
      "Iteration 780, loss = 0.09198096\n",
      "Iteration 781, loss = 0.09153401\n",
      "Iteration 782, loss = 0.09109109\n",
      "Iteration 783, loss = 0.09065173\n",
      "Iteration 784, loss = 0.09018989\n",
      "Iteration 785, loss = 0.08975537\n",
      "Iteration 786, loss = 0.08932010\n",
      "Iteration 787, loss = 0.08889944\n",
      "Iteration 788, loss = 0.08844474\n",
      "Iteration 789, loss = 0.08805505\n",
      "Iteration 790, loss = 0.08757513\n",
      "Iteration 791, loss = 0.08718002\n",
      "Iteration 792, loss = 0.08672487\n",
      "Iteration 793, loss = 0.08632301\n",
      "Iteration 794, loss = 0.08589130\n",
      "Iteration 795, loss = 0.08547312\n",
      "Iteration 796, loss = 0.08504396\n",
      "Iteration 797, loss = 0.08464562\n",
      "Iteration 798, loss = 0.08418854\n",
      "Iteration 799, loss = 0.08374201\n",
      "Iteration 800, loss = 0.08331687\n",
      "Iteration 801, loss = 0.08289279\n",
      "Iteration 802, loss = 0.08248557\n",
      "Iteration 803, loss = 0.08207922\n",
      "Iteration 804, loss = 0.08166710\n",
      "Iteration 805, loss = 0.08127417\n",
      "Iteration 806, loss = 0.08088458\n",
      "Iteration 807, loss = 0.08047530\n",
      "Iteration 808, loss = 0.08008289\n",
      "Iteration 809, loss = 0.07970574\n",
      "Iteration 810, loss = 0.07932227\n",
      "Iteration 811, loss = 0.07893579\n",
      "Iteration 812, loss = 0.07854551\n",
      "Iteration 813, loss = 0.07817687\n",
      "Iteration 814, loss = 0.07780978\n",
      "Iteration 815, loss = 0.07742494\n",
      "Iteration 816, loss = 0.07705820\n",
      "Iteration 817, loss = 0.07668938\n",
      "Iteration 818, loss = 0.07634201\n",
      "Iteration 819, loss = 0.07599046\n",
      "Iteration 820, loss = 0.07561414\n",
      "Iteration 821, loss = 0.07525651\n",
      "Iteration 822, loss = 0.07492113\n",
      "Iteration 823, loss = 0.07457048\n",
      "Iteration 824, loss = 0.07424477\n",
      "Iteration 825, loss = 0.07389780\n",
      "Iteration 826, loss = 0.07356388\n",
      "Iteration 827, loss = 0.07323309\n",
      "Iteration 828, loss = 0.07291037\n",
      "Iteration 829, loss = 0.07258633\n",
      "Iteration 830, loss = 0.07227867\n",
      "Iteration 831, loss = 0.07194836\n",
      "Iteration 832, loss = 0.07159321\n",
      "Iteration 833, loss = 0.07129109\n",
      "Iteration 834, loss = 0.07096840\n",
      "Iteration 835, loss = 0.07065000\n",
      "Iteration 836, loss = 0.07037302\n",
      "Iteration 837, loss = 0.07002837\n",
      "Iteration 838, loss = 0.06973591\n",
      "Iteration 839, loss = 0.06943806\n",
      "Iteration 840, loss = 0.06911210\n",
      "Iteration 841, loss = 0.06882042\n",
      "Iteration 842, loss = 0.06852278\n",
      "Iteration 843, loss = 0.06821474\n",
      "Iteration 844, loss = 0.06790453\n",
      "Iteration 845, loss = 0.06763694\n",
      "Iteration 846, loss = 0.06732856\n",
      "Iteration 847, loss = 0.06706181\n",
      "Iteration 848, loss = 0.06676351\n",
      "Iteration 849, loss = 0.06648557\n",
      "Iteration 850, loss = 0.06620154\n",
      "Iteration 851, loss = 0.06591767\n",
      "Iteration 852, loss = 0.06566653\n",
      "Iteration 853, loss = 0.06536716\n",
      "Iteration 854, loss = 0.06512788\n",
      "Iteration 855, loss = 0.06484970\n",
      "Iteration 856, loss = 0.06454934\n",
      "Iteration 857, loss = 0.06427390\n",
      "Iteration 858, loss = 0.06401787\n",
      "Iteration 859, loss = 0.06373306\n",
      "Iteration 860, loss = 0.06344168\n",
      "Iteration 861, loss = 0.06317144\n",
      "Iteration 862, loss = 0.06290615\n",
      "Iteration 863, loss = 0.06264720\n",
      "Iteration 864, loss = 0.06236815\n",
      "Iteration 865, loss = 0.06209585\n",
      "Iteration 866, loss = 0.06181543\n",
      "Iteration 867, loss = 0.06155328\n",
      "Iteration 868, loss = 0.06129013\n",
      "Iteration 869, loss = 0.06103484\n",
      "Iteration 870, loss = 0.06078065\n",
      "Iteration 871, loss = 0.06053062\n",
      "Iteration 872, loss = 0.06026524\n",
      "Iteration 873, loss = 0.06000614\n",
      "Iteration 874, loss = 0.05977404\n",
      "Iteration 875, loss = 0.05951127\n",
      "Iteration 876, loss = 0.05925778\n",
      "Iteration 877, loss = 0.05902045\n",
      "Iteration 878, loss = 0.05876224\n",
      "Iteration 879, loss = 0.05853100\n",
      "Iteration 880, loss = 0.05828559\n",
      "Iteration 881, loss = 0.05804039\n",
      "Iteration 882, loss = 0.05776743\n",
      "Iteration 883, loss = 0.05753698\n",
      "Iteration 884, loss = 0.05730945\n",
      "Iteration 885, loss = 0.05706686\n",
      "Iteration 886, loss = 0.05685077\n",
      "Iteration 887, loss = 0.05660239\n",
      "Iteration 888, loss = 0.05635604\n",
      "Iteration 889, loss = 0.05611702\n",
      "Iteration 890, loss = 0.05594455\n",
      "Iteration 891, loss = 0.05570863\n",
      "Iteration 892, loss = 0.05541935\n",
      "Iteration 893, loss = 0.05519013\n",
      "Iteration 894, loss = 0.05496402\n",
      "Iteration 895, loss = 0.05474636\n",
      "Iteration 896, loss = 0.05453658\n",
      "Iteration 897, loss = 0.05432714\n",
      "Iteration 898, loss = 0.05409978\n",
      "Iteration 899, loss = 0.05388771\n",
      "Iteration 900, loss = 0.05364612\n",
      "Iteration 901, loss = 0.05343677\n",
      "Iteration 902, loss = 0.05319920\n",
      "Iteration 903, loss = 0.05298895\n",
      "Iteration 904, loss = 0.05278816\n",
      "Iteration 905, loss = 0.05261820\n",
      "Iteration 906, loss = 0.05237663\n",
      "Iteration 907, loss = 0.05214102\n",
      "Iteration 908, loss = 0.05192895\n",
      "Iteration 909, loss = 0.05172965\n",
      "Iteration 910, loss = 0.05154113\n",
      "Iteration 911, loss = 0.05130810\n",
      "Iteration 912, loss = 0.05113834\n",
      "Iteration 913, loss = 0.05090105\n",
      "Iteration 914, loss = 0.05069046\n",
      "Iteration 915, loss = 0.05050589\n",
      "Iteration 916, loss = 0.05031689\n",
      "Iteration 917, loss = 0.05013709\n",
      "Iteration 918, loss = 0.04990678\n",
      "Iteration 919, loss = 0.04971924\n",
      "Iteration 920, loss = 0.04953187\n",
      "Iteration 921, loss = 0.04931794\n",
      "Iteration 922, loss = 0.04913475\n",
      "Iteration 923, loss = 0.04894639\n",
      "Iteration 924, loss = 0.04872363\n",
      "Iteration 925, loss = 0.04857341\n",
      "Iteration 926, loss = 0.04838141\n",
      "Iteration 927, loss = 0.04817833\n",
      "Iteration 928, loss = 0.04800080\n",
      "Iteration 929, loss = 0.04783608\n",
      "Iteration 930, loss = 0.04762414\n",
      "Iteration 931, loss = 0.04741716\n",
      "Iteration 932, loss = 0.04729054\n",
      "Iteration 933, loss = 0.04706738\n",
      "Iteration 934, loss = 0.04691243\n",
      "Iteration 935, loss = 0.04674174\n",
      "Iteration 936, loss = 0.04652331\n",
      "Iteration 937, loss = 0.04636632\n",
      "Iteration 938, loss = 0.04618981\n",
      "Iteration 939, loss = 0.04601517\n",
      "Iteration 940, loss = 0.04582601\n",
      "Iteration 941, loss = 0.04567446\n",
      "Iteration 942, loss = 0.04551535\n",
      "Iteration 943, loss = 0.04533844\n",
      "Iteration 944, loss = 0.04514145\n",
      "Iteration 945, loss = 0.04496295\n",
      "Iteration 946, loss = 0.04481740\n",
      "Iteration 947, loss = 0.04464438\n",
      "Iteration 948, loss = 0.04447574\n",
      "Iteration 949, loss = 0.04431075\n",
      "Iteration 950, loss = 0.04413647\n",
      "Iteration 951, loss = 0.04398855\n",
      "Iteration 952, loss = 0.04381036\n",
      "Iteration 953, loss = 0.04367720\n",
      "Iteration 954, loss = 0.04351370\n",
      "Iteration 955, loss = 0.04331310\n",
      "Iteration 956, loss = 0.04319750\n",
      "Iteration 957, loss = 0.04302125\n",
      "Iteration 958, loss = 0.04283980\n",
      "Iteration 959, loss = 0.04269829\n",
      "Iteration 960, loss = 0.04257184\n",
      "Iteration 961, loss = 0.04238047\n",
      "Iteration 962, loss = 0.04220596\n",
      "Iteration 963, loss = 0.04205827\n",
      "Iteration 964, loss = 0.04191773\n",
      "Iteration 965, loss = 0.04176802\n",
      "Iteration 966, loss = 0.04161485\n",
      "Iteration 967, loss = 0.04146753\n",
      "Iteration 968, loss = 0.04130996\n",
      "Iteration 969, loss = 0.04120161\n",
      "Iteration 970, loss = 0.04103105\n",
      "Iteration 971, loss = 0.04087013\n",
      "Iteration 972, loss = 0.04073460\n",
      "Iteration 973, loss = 0.04058754\n",
      "Iteration 974, loss = 0.04042761\n",
      "Iteration 975, loss = 0.04028262\n",
      "Iteration 976, loss = 0.04013109\n",
      "Iteration 977, loss = 0.04000661\n",
      "Iteration 978, loss = 0.03984124\n",
      "Iteration 979, loss = 0.03969812\n",
      "Iteration 980, loss = 0.03956200\n",
      "Iteration 981, loss = 0.03942307\n",
      "Iteration 982, loss = 0.03931681\n",
      "Iteration 983, loss = 0.03916164\n",
      "Iteration 984, loss = 0.03903524\n",
      "Iteration 985, loss = 0.03889897\n",
      "Iteration 986, loss = 0.03874436\n",
      "Iteration 987, loss = 0.03859696\n",
      "Iteration 988, loss = 0.03845282\n",
      "Iteration 989, loss = 0.03833841\n",
      "Iteration 990, loss = 0.03820086\n",
      "Iteration 991, loss = 0.03805583\n",
      "Iteration 992, loss = 0.03791954\n",
      "Iteration 993, loss = 0.03782205\n",
      "Iteration 994, loss = 0.03762615\n",
      "Iteration 995, loss = 0.03749948\n",
      "Iteration 996, loss = 0.03735580\n",
      "Iteration 997, loss = 0.03723146\n",
      "Iteration 998, loss = 0.03709963\n",
      "Iteration 999, loss = 0.03696707\n",
      "Iteration 1000, loss = 0.03683481\n",
      "Iteration 1001, loss = 0.03669965\n",
      "Iteration 1002, loss = 0.03657702\n",
      "Iteration 1003, loss = 0.03644271\n",
      "Iteration 1004, loss = 0.03630434\n",
      "Iteration 1005, loss = 0.03618043\n",
      "Iteration 1006, loss = 0.03606654\n",
      "Iteration 1007, loss = 0.03593838\n",
      "Iteration 1008, loss = 0.03579755\n",
      "Iteration 1009, loss = 0.03569057\n",
      "Iteration 1010, loss = 0.03554651\n",
      "Iteration 1011, loss = 0.03543867\n",
      "Iteration 1012, loss = 0.03529206\n",
      "Iteration 1013, loss = 0.03515920\n",
      "Iteration 1014, loss = 0.03507698\n",
      "Iteration 1015, loss = 0.03494132\n",
      "Iteration 1016, loss = 0.03481065\n",
      "Iteration 1017, loss = 0.03470360\n",
      "Iteration 1018, loss = 0.03455741\n",
      "Iteration 1019, loss = 0.03444549\n",
      "Iteration 1020, loss = 0.03435363\n",
      "Iteration 1021, loss = 0.03419817\n",
      "Iteration 1022, loss = 0.03410406\n",
      "Iteration 1023, loss = 0.03397995\n",
      "Iteration 1024, loss = 0.03385479\n",
      "Iteration 1025, loss = 0.03372984\n",
      "Iteration 1026, loss = 0.03361595\n",
      "Iteration 1027, loss = 0.03350857\n",
      "Iteration 1028, loss = 0.03337453\n",
      "Iteration 1029, loss = 0.03326196\n",
      "Iteration 1030, loss = 0.03314097\n",
      "Iteration 1031, loss = 0.03303154\n",
      "Iteration 1032, loss = 0.03291999\n",
      "Iteration 1033, loss = 0.03281183\n",
      "Iteration 1034, loss = 0.03270409\n",
      "Iteration 1035, loss = 0.03258740\n",
      "Iteration 1036, loss = 0.03248321\n",
      "Iteration 1037, loss = 0.03242044\n",
      "Iteration 1038, loss = 0.03225907\n",
      "Iteration 1039, loss = 0.03218284\n",
      "Iteration 1040, loss = 0.03202686\n",
      "Iteration 1041, loss = 0.03196784\n",
      "Iteration 1042, loss = 0.03181431\n",
      "Iteration 1043, loss = 0.03170873\n",
      "Iteration 1044, loss = 0.03163086\n",
      "Iteration 1045, loss = 0.03148467\n",
      "Iteration 1046, loss = 0.03136931\n",
      "Iteration 1047, loss = 0.03128518\n",
      "Iteration 1048, loss = 0.03120197\n",
      "Iteration 1049, loss = 0.03106317\n",
      "Iteration 1050, loss = 0.03098636\n",
      "Iteration 1051, loss = 0.03091078\n",
      "Iteration 1052, loss = 0.03073165\n",
      "Iteration 1053, loss = 0.03066166\n",
      "Iteration 1054, loss = 0.03054417\n",
      "Iteration 1055, loss = 0.03042968\n",
      "Iteration 1056, loss = 0.03032785\n",
      "Iteration 1057, loss = 0.03023119\n",
      "Iteration 1058, loss = 0.03012007\n",
      "Iteration 1059, loss = 0.03001851\n",
      "Iteration 1060, loss = 0.02994608\n",
      "Iteration 1061, loss = 0.02985010\n",
      "Iteration 1062, loss = 0.02971271\n",
      "Iteration 1063, loss = 0.02966312\n",
      "Iteration 1064, loss = 0.02953383\n",
      "Iteration 1065, loss = 0.02942343\n",
      "Iteration 1066, loss = 0.02933440\n",
      "Iteration 1067, loss = 0.02924502\n",
      "Iteration 1068, loss = 0.02917249\n",
      "Iteration 1069, loss = 0.02906078\n",
      "Iteration 1070, loss = 0.02894493\n",
      "Iteration 1071, loss = 0.02887424\n",
      "Iteration 1072, loss = 0.02875670\n",
      "Iteration 1073, loss = 0.02867217\n",
      "Iteration 1074, loss = 0.02853648\n",
      "Iteration 1075, loss = 0.02845723\n",
      "Iteration 1076, loss = 0.02837987\n",
      "Iteration 1077, loss = 0.02826631\n",
      "Iteration 1078, loss = 0.02819269\n",
      "Iteration 1079, loss = 0.02807540\n",
      "Iteration 1080, loss = 0.02800238\n",
      "Iteration 1081, loss = 0.02791316\n",
      "Iteration 1082, loss = 0.02780718\n",
      "Iteration 1083, loss = 0.02771527\n",
      "Iteration 1084, loss = 0.02762777\n",
      "Iteration 1085, loss = 0.02752385\n",
      "Iteration 1086, loss = 0.02742962\n",
      "Iteration 1087, loss = 0.02733487\n",
      "Iteration 1088, loss = 0.02728753\n",
      "Iteration 1089, loss = 0.02718416\n",
      "Iteration 1090, loss = 0.02709034\n",
      "Iteration 1091, loss = 0.02698626\n",
      "Iteration 1092, loss = 0.02689999\n",
      "Iteration 1093, loss = 0.02681465\n",
      "Iteration 1094, loss = 0.02672602\n",
      "Iteration 1095, loss = 0.02667800\n",
      "Iteration 1096, loss = 0.02656444\n",
      "Iteration 1097, loss = 0.02651520\n",
      "Iteration 1098, loss = 0.02639068\n",
      "Iteration 1099, loss = 0.02629930\n",
      "Iteration 1100, loss = 0.02621664\n",
      "Iteration 1101, loss = 0.02613012\n",
      "Iteration 1102, loss = 0.02604380\n",
      "Iteration 1103, loss = 0.02596020\n",
      "Iteration 1104, loss = 0.02588411\n",
      "Iteration 1105, loss = 0.02580795\n",
      "Iteration 1106, loss = 0.02572345\n",
      "Iteration 1107, loss = 0.02562444\n",
      "Iteration 1108, loss = 0.02555228\n",
      "Iteration 1109, loss = 0.02545969\n",
      "Iteration 1110, loss = 0.02538346\n",
      "Iteration 1111, loss = 0.02529551\n",
      "Iteration 1112, loss = 0.02520279\n",
      "Iteration 1113, loss = 0.02514623\n",
      "Iteration 1114, loss = 0.02504718\n",
      "Iteration 1115, loss = 0.02498298\n",
      "Iteration 1116, loss = 0.02491251\n",
      "Iteration 1117, loss = 0.02483093\n",
      "Iteration 1118, loss = 0.02473683\n",
      "Iteration 1119, loss = 0.02467740\n",
      "Iteration 1120, loss = 0.02458754\n",
      "Iteration 1121, loss = 0.02448667\n",
      "Iteration 1122, loss = 0.02440065\n",
      "Iteration 1123, loss = 0.02433129\n",
      "Iteration 1124, loss = 0.02425774\n",
      "Iteration 1125, loss = 0.02419808\n",
      "Iteration 1126, loss = 0.02412235\n",
      "Iteration 1127, loss = 0.02402229\n",
      "Iteration 1128, loss = 0.02395691\n",
      "Iteration 1129, loss = 0.02387178\n",
      "Iteration 1130, loss = 0.02380943\n",
      "Iteration 1131, loss = 0.02372700\n",
      "Iteration 1132, loss = 0.02366656\n",
      "Iteration 1133, loss = 0.02356544\n",
      "Iteration 1134, loss = 0.02348701\n",
      "Iteration 1135, loss = 0.02341835\n",
      "Iteration 1136, loss = 0.02335333\n",
      "Iteration 1137, loss = 0.02327784\n",
      "Iteration 1138, loss = 0.02320654\n",
      "Iteration 1139, loss = 0.02314087\n",
      "Iteration 1140, loss = 0.02306457\n",
      "Iteration 1141, loss = 0.02298839\n",
      "Iteration 1142, loss = 0.02291776\n",
      "Iteration 1143, loss = 0.02285830\n",
      "Iteration 1144, loss = 0.02279431\n",
      "Iteration 1145, loss = 0.02273140\n",
      "Iteration 1146, loss = 0.02265549\n",
      "Iteration 1147, loss = 0.02257639\n",
      "Iteration 1148, loss = 0.02248269\n",
      "Iteration 1149, loss = 0.02242186\n",
      "Iteration 1150, loss = 0.02234094\n",
      "Iteration 1151, loss = 0.02227745\n",
      "Iteration 1152, loss = 0.02221025\n",
      "Iteration 1153, loss = 0.02213225\n",
      "Iteration 1154, loss = 0.02206544\n",
      "Iteration 1155, loss = 0.02199014\n",
      "Iteration 1156, loss = 0.02193265\n",
      "Iteration 1157, loss = 0.02186262\n",
      "Iteration 1158, loss = 0.02178585\n",
      "Iteration 1159, loss = 0.02171916\n",
      "Iteration 1160, loss = 0.02165232\n",
      "Iteration 1161, loss = 0.02160408\n",
      "Iteration 1162, loss = 0.02154028\n",
      "Iteration 1163, loss = 0.02144788\n",
      "Iteration 1164, loss = 0.02137382\n",
      "Iteration 1165, loss = 0.02132196\n",
      "Iteration 1166, loss = 0.02124668\n",
      "Iteration 1167, loss = 0.02118496\n",
      "Iteration 1168, loss = 0.02111947\n",
      "Iteration 1169, loss = 0.02106565\n",
      "Iteration 1170, loss = 0.02098043\n",
      "Iteration 1171, loss = 0.02094604\n",
      "Iteration 1172, loss = 0.02087509\n",
      "Iteration 1173, loss = 0.02078627\n",
      "Iteration 1174, loss = 0.02075651\n",
      "Iteration 1175, loss = 0.02070438\n",
      "Iteration 1176, loss = 0.02059741\n",
      "Iteration 1177, loss = 0.02054613\n",
      "Iteration 1178, loss = 0.02046539\n",
      "Iteration 1179, loss = 0.02041346\n",
      "Iteration 1180, loss = 0.02037153\n",
      "Iteration 1181, loss = 0.02029097\n",
      "Iteration 1182, loss = 0.02022306\n",
      "Iteration 1183, loss = 0.02015789\n",
      "Iteration 1184, loss = 0.02010507\n",
      "Iteration 1185, loss = 0.02002293\n",
      "Iteration 1186, loss = 0.01997018\n",
      "Iteration 1187, loss = 0.01992556\n",
      "Iteration 1188, loss = 0.01985307\n",
      "Iteration 1189, loss = 0.01979222\n",
      "Iteration 1190, loss = 0.01972602\n",
      "Iteration 1191, loss = 0.01969945\n",
      "Iteration 1192, loss = 0.01961141\n",
      "Iteration 1193, loss = 0.01954836\n",
      "Iteration 1194, loss = 0.01946134\n",
      "Iteration 1195, loss = 0.01941784\n",
      "Iteration 1196, loss = 0.01936953\n",
      "Iteration 1197, loss = 0.01930184\n",
      "Iteration 1198, loss = 0.01923747\n",
      "Iteration 1199, loss = 0.01920731\n",
      "Iteration 1200, loss = 0.01915508\n",
      "Iteration 1201, loss = 0.01905930\n",
      "Iteration 1202, loss = 0.01902826\n",
      "Iteration 1203, loss = 0.01894874\n",
      "Iteration 1204, loss = 0.01888814\n",
      "Iteration 1205, loss = 0.01880926\n",
      "Iteration 1206, loss = 0.01880377\n",
      "Iteration 1207, loss = 0.01873383\n",
      "Iteration 1208, loss = 0.01867579\n",
      "Iteration 1209, loss = 0.01860743\n",
      "Iteration 1210, loss = 0.01853159\n",
      "Iteration 1211, loss = 0.01847730\n",
      "Iteration 1212, loss = 0.01843473\n",
      "Iteration 1213, loss = 0.01838905\n",
      "Iteration 1214, loss = 0.01833358\n",
      "Iteration 1215, loss = 0.01828348\n",
      "Iteration 1216, loss = 0.01819805\n",
      "Iteration 1217, loss = 0.01819956\n",
      "Iteration 1218, loss = 0.01811838\n",
      "Iteration 1219, loss = 0.01807356\n",
      "Iteration 1220, loss = 0.01800407\n",
      "Iteration 1221, loss = 0.01793200\n",
      "Iteration 1222, loss = 0.01789695\n",
      "Iteration 1223, loss = 0.01783576\n",
      "Iteration 1224, loss = 0.01776976\n",
      "Iteration 1225, loss = 0.01770131\n",
      "Iteration 1226, loss = 0.01764512\n",
      "Iteration 1227, loss = 0.01760136\n",
      "Iteration 1228, loss = 0.01758760\n",
      "Iteration 1229, loss = 0.01749025\n",
      "Iteration 1230, loss = 0.01743107\n",
      "Iteration 1231, loss = 0.01743188\n",
      "Iteration 1232, loss = 0.01738726\n",
      "Iteration 1233, loss = 0.01729665\n",
      "Iteration 1234, loss = 0.01723323\n",
      "Iteration 1235, loss = 0.01719757\n",
      "Iteration 1236, loss = 0.01713235\n",
      "Iteration 1237, loss = 0.01706410\n",
      "Iteration 1238, loss = 0.01704309\n",
      "Iteration 1239, loss = 0.01697017\n",
      "Iteration 1240, loss = 0.01692852\n",
      "Iteration 1241, loss = 0.01685360\n",
      "Iteration 1242, loss = 0.01679754\n",
      "Iteration 1243, loss = 0.01674685\n",
      "Iteration 1244, loss = 0.01670267\n",
      "Iteration 1245, loss = 0.01667701\n",
      "Iteration 1246, loss = 0.01667155\n",
      "Iteration 1247, loss = 0.01656180\n",
      "Iteration 1248, loss = 0.01650383\n",
      "Iteration 1249, loss = 0.01646574\n",
      "Iteration 1250, loss = 0.01640767\n",
      "Iteration 1251, loss = 0.01636977\n",
      "Iteration 1252, loss = 0.01631590\n",
      "Iteration 1253, loss = 0.01625571\n",
      "Iteration 1254, loss = 0.01625132\n",
      "Iteration 1255, loss = 0.01616655\n",
      "Iteration 1256, loss = 0.01610426\n",
      "Iteration 1257, loss = 0.01605928\n",
      "Iteration 1258, loss = 0.01605503\n",
      "Iteration 1259, loss = 0.01597374\n",
      "Iteration 1260, loss = 0.01592290\n",
      "Iteration 1261, loss = 0.01587589\n",
      "Iteration 1262, loss = 0.01584543\n",
      "Iteration 1263, loss = 0.01577435\n",
      "Iteration 1264, loss = 0.01571795\n",
      "Iteration 1265, loss = 0.01567667\n",
      "Iteration 1266, loss = 0.01562355\n",
      "Iteration 1267, loss = 0.01562427\n",
      "Iteration 1268, loss = 0.01552526\n",
      "Iteration 1269, loss = 0.01548259\n",
      "Iteration 1270, loss = 0.01547933\n",
      "Iteration 1271, loss = 0.01542915\n",
      "Iteration 1272, loss = 0.01537469\n",
      "Iteration 1273, loss = 0.01532887\n",
      "Iteration 1274, loss = 0.01526744\n",
      "Iteration 1275, loss = 0.01521218\n",
      "Iteration 1276, loss = 0.01517743\n",
      "Iteration 1277, loss = 0.01515090\n",
      "Iteration 1278, loss = 0.01508080\n",
      "Iteration 1279, loss = 0.01505036\n",
      "Iteration 1280, loss = 0.01500946\n",
      "Iteration 1281, loss = 0.01495581\n",
      "Iteration 1282, loss = 0.01490175\n",
      "Iteration 1283, loss = 0.01486543\n",
      "Iteration 1284, loss = 0.01481928\n",
      "Iteration 1285, loss = 0.01477125\n",
      "Iteration 1286, loss = 0.01473277\n",
      "Iteration 1287, loss = 0.01468432\n",
      "Iteration 1288, loss = 0.01465331\n",
      "Iteration 1289, loss = 0.01461255\n",
      "Iteration 1290, loss = 0.01455727\n",
      "Iteration 1291, loss = 0.01456112\n",
      "Iteration 1292, loss = 0.01448098\n",
      "Iteration 1293, loss = 0.01441910\n",
      "Iteration 1294, loss = 0.01444155\n",
      "Iteration 1295, loss = 0.01441608\n",
      "Iteration 1296, loss = 0.01430310\n",
      "Iteration 1297, loss = 0.01427536\n",
      "Iteration 1298, loss = 0.01421540\n",
      "Iteration 1299, loss = 0.01418655\n",
      "Iteration 1300, loss = 0.01415059\n",
      "Iteration 1301, loss = 0.01410448\n",
      "Iteration 1302, loss = 0.01407494\n",
      "Iteration 1303, loss = 0.01402322\n",
      "Iteration 1304, loss = 0.01397163\n",
      "Iteration 1305, loss = 0.01400803\n",
      "Iteration 1306, loss = 0.01391103\n",
      "Iteration 1307, loss = 0.01386766\n",
      "Iteration 1308, loss = 0.01382709\n",
      "Iteration 1309, loss = 0.01378188\n",
      "Iteration 1310, loss = 0.01375117\n",
      "Iteration 1311, loss = 0.01372157\n",
      "Iteration 1312, loss = 0.01369074\n",
      "Iteration 1313, loss = 0.01362846\n",
      "Iteration 1314, loss = 0.01364707\n",
      "Iteration 1315, loss = 0.01358663\n",
      "Iteration 1316, loss = 0.01351736\n",
      "Iteration 1317, loss = 0.01347220\n",
      "Iteration 1318, loss = 0.01342975\n",
      "Iteration 1319, loss = 0.01339696\n",
      "Iteration 1320, loss = 0.01334650\n",
      "Iteration 1321, loss = 0.01331232\n",
      "Iteration 1322, loss = 0.01326877\n",
      "Iteration 1323, loss = 0.01326828\n",
      "Iteration 1324, loss = 0.01324501\n",
      "Iteration 1325, loss = 0.01317304\n",
      "Iteration 1326, loss = 0.01314107\n",
      "Iteration 1327, loss = 0.01307670\n",
      "Iteration 1328, loss = 0.01304692\n",
      "Iteration 1329, loss = 0.01302106\n",
      "Iteration 1330, loss = 0.01299669\n",
      "Iteration 1331, loss = 0.01294745\n",
      "Iteration 1332, loss = 0.01289727\n",
      "Iteration 1333, loss = 0.01285731\n",
      "Iteration 1334, loss = 0.01284054\n",
      "Iteration 1335, loss = 0.01279627\n",
      "Iteration 1336, loss = 0.01277095\n",
      "Iteration 1337, loss = 0.01273356\n",
      "Iteration 1338, loss = 0.01268062\n",
      "Iteration 1339, loss = 0.01264041\n",
      "Iteration 1340, loss = 0.01262061\n",
      "Iteration 1341, loss = 0.01258260\n",
      "Iteration 1342, loss = 0.01255910\n",
      "Iteration 1343, loss = 0.01254867\n",
      "Iteration 1344, loss = 0.01247086\n",
      "Iteration 1345, loss = 0.01242835\n",
      "Iteration 1346, loss = 0.01242037\n",
      "Iteration 1347, loss = 0.01237927\n",
      "Iteration 1348, loss = 0.01233829\n",
      "Iteration 1349, loss = 0.01229160\n",
      "Iteration 1350, loss = 0.01225774\n",
      "Iteration 1351, loss = 0.01222157\n",
      "Iteration 1352, loss = 0.01221674\n",
      "Iteration 1353, loss = 0.01219995\n",
      "Iteration 1354, loss = 0.01213881\n",
      "Iteration 1355, loss = 0.01208538\n",
      "Iteration 1356, loss = 0.01207915\n",
      "Iteration 1357, loss = 0.01202692\n",
      "Iteration 1358, loss = 0.01199024\n",
      "Iteration 1359, loss = 0.01194983\n",
      "Iteration 1360, loss = 0.01192251\n",
      "Iteration 1361, loss = 0.01188930\n",
      "Iteration 1362, loss = 0.01185944\n",
      "Iteration 1363, loss = 0.01182523\n",
      "Iteration 1364, loss = 0.01178689\n",
      "Iteration 1365, loss = 0.01174917\n",
      "Iteration 1366, loss = 0.01173176\n",
      "Iteration 1367, loss = 0.01171566\n",
      "Iteration 1368, loss = 0.01167523\n",
      "Iteration 1369, loss = 0.01162990\n",
      "Iteration 1370, loss = 0.01161138\n",
      "Iteration 1371, loss = 0.01155865\n",
      "Iteration 1372, loss = 0.01153130\n",
      "Iteration 1373, loss = 0.01149477\n",
      "Iteration 1374, loss = 0.01146497\n",
      "Iteration 1375, loss = 0.01143273\n",
      "Iteration 1376, loss = 0.01145030\n",
      "Iteration 1377, loss = 0.01137823\n",
      "Iteration 1378, loss = 0.01134015\n",
      "Iteration 1379, loss = 0.01130642\n",
      "Iteration 1380, loss = 0.01127549\n",
      "Iteration 1381, loss = 0.01124257\n",
      "Iteration 1382, loss = 0.01122751\n",
      "Iteration 1383, loss = 0.01118171\n",
      "Iteration 1384, loss = 0.01117175\n",
      "Iteration 1385, loss = 0.01111316\n",
      "Iteration 1386, loss = 0.01110493\n",
      "Iteration 1387, loss = 0.01106016\n",
      "Iteration 1388, loss = 0.01107082\n",
      "Iteration 1389, loss = 0.01100102\n",
      "Iteration 1390, loss = 0.01099428\n",
      "Iteration 1391, loss = 0.01094405\n",
      "Iteration 1392, loss = 0.01091435\n",
      "Iteration 1393, loss = 0.01092326\n",
      "Iteration 1394, loss = 0.01085549\n",
      "Iteration 1395, loss = 0.01081419\n",
      "Iteration 1396, loss = 0.01081819\n",
      "Iteration 1397, loss = 0.01076431\n",
      "Iteration 1398, loss = 0.01074478\n",
      "Iteration 1399, loss = 0.01069543\n",
      "Iteration 1400, loss = 0.01070422\n",
      "Iteration 1401, loss = 0.01064146\n",
      "Iteration 1402, loss = 0.01061015\n",
      "Iteration 1403, loss = 0.01059244\n",
      "Iteration 1404, loss = 0.01054798\n",
      "Iteration 1405, loss = 0.01057297\n",
      "Iteration 1406, loss = 0.01050435\n",
      "Iteration 1407, loss = 0.01047244\n",
      "Iteration 1408, loss = 0.01047283\n",
      "Iteration 1409, loss = 0.01041276\n",
      "Iteration 1410, loss = 0.01039563\n",
      "Iteration 1411, loss = 0.01038233\n",
      "Iteration 1412, loss = 0.01034793\n",
      "Iteration 1413, loss = 0.01029356\n",
      "Iteration 1414, loss = 0.01030322\n",
      "Iteration 1415, loss = 0.01025794\n",
      "Iteration 1416, loss = 0.01023197\n",
      "Iteration 1417, loss = 0.01022549\n",
      "Iteration 1418, loss = 0.01016342\n",
      "Iteration 1419, loss = 0.01014086\n",
      "Iteration 1420, loss = 0.01013116\n",
      "Iteration 1421, loss = 0.01012602\n",
      "Iteration 1422, loss = 0.01006339\n",
      "Iteration 1423, loss = 0.01003689\n",
      "Iteration 1424, loss = 0.01002949\n",
      "Iteration 1425, loss = 0.01002502\n",
      "Iteration 1426, loss = 0.00995478\n",
      "Iteration 1427, loss = 0.00996178\n",
      "Iteration 1428, loss = 0.00990116\n",
      "Iteration 1429, loss = 0.00990713\n",
      "Iteration 1430, loss = 0.00986323\n",
      "Iteration 1431, loss = 0.00983314\n",
      "Iteration 1432, loss = 0.00981543\n",
      "Iteration 1433, loss = 0.00977072\n",
      "Iteration 1434, loss = 0.00977073\n",
      "Iteration 1435, loss = 0.00970105\n",
      "Iteration 1436, loss = 0.00971029\n",
      "Iteration 1437, loss = 0.00966705\n",
      "Iteration 1438, loss = 0.00965422\n",
      "Iteration 1439, loss = 0.00963130\n",
      "Iteration 1440, loss = 0.00959211\n",
      "Iteration 1441, loss = 0.00959785\n",
      "Iteration 1442, loss = 0.00953516\n",
      "Iteration 1443, loss = 0.00950444\n",
      "Iteration 1444, loss = 0.00950836\n",
      "Iteration 1445, loss = 0.00947112\n",
      "Iteration 1446, loss = 0.00943306\n",
      "Iteration 1447, loss = 0.00941862\n",
      "Iteration 1448, loss = 0.00940217\n",
      "Iteration 1449, loss = 0.00938068\n",
      "Iteration 1450, loss = 0.00934021\n",
      "Iteration 1451, loss = 0.00932065\n",
      "Iteration 1452, loss = 0.00929789\n",
      "Iteration 1453, loss = 0.00927339\n",
      "Iteration 1454, loss = 0.00925102\n",
      "Iteration 1455, loss = 0.00924086\n",
      "Iteration 1456, loss = 0.00921867\n",
      "Iteration 1457, loss = 0.00917762\n",
      "Iteration 1458, loss = 0.00914919\n",
      "Iteration 1459, loss = 0.00913134\n",
      "Iteration 1460, loss = 0.00910420\n",
      "Iteration 1461, loss = 0.00909462\n",
      "Iteration 1462, loss = 0.00907923\n",
      "Iteration 1463, loss = 0.00904250\n",
      "Iteration 1464, loss = 0.00902438\n",
      "Iteration 1465, loss = 0.00897329\n",
      "Iteration 1466, loss = 0.00900088\n",
      "Iteration 1467, loss = 0.00895564\n",
      "Iteration 1468, loss = 0.00893721\n",
      "Iteration 1469, loss = 0.00890503\n",
      "Iteration 1470, loss = 0.00887665\n",
      "Iteration 1471, loss = 0.00885809\n",
      "Iteration 1472, loss = 0.00886369\n",
      "Iteration 1473, loss = 0.00882559\n",
      "Iteration 1474, loss = 0.00879518\n",
      "Iteration 1475, loss = 0.00877343\n",
      "Iteration 1476, loss = 0.00876675\n",
      "Iteration 1477, loss = 0.00874022\n",
      "Iteration 1478, loss = 0.00872509\n",
      "Iteration 1479, loss = 0.00870198\n",
      "Iteration 1480, loss = 0.00866251\n",
      "Iteration 1481, loss = 0.00864556\n",
      "Iteration 1482, loss = 0.00865307\n",
      "Iteration 1483, loss = 0.00861035\n",
      "Iteration 1484, loss = 0.00856134\n",
      "Iteration 1485, loss = 0.00855713\n",
      "Iteration 1486, loss = 0.00851688\n",
      "Iteration 1487, loss = 0.00854155\n",
      "Iteration 1488, loss = 0.00846404\n",
      "Iteration 1489, loss = 0.00845629\n",
      "Iteration 1490, loss = 0.00847160\n",
      "Iteration 1491, loss = 0.00843943\n",
      "Iteration 1492, loss = 0.00842139\n",
      "Iteration 1493, loss = 0.00838611\n",
      "Iteration 1494, loss = 0.00834705\n",
      "Iteration 1495, loss = 0.00833103\n",
      "Iteration 1496, loss = 0.00829933\n",
      "Iteration 1497, loss = 0.00828347\n",
      "Iteration 1498, loss = 0.00827988\n",
      "Iteration 1499, loss = 0.00827553\n",
      "Iteration 1500, loss = 0.00823658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mbern\\OneDrive\\machineLearningCourse\\courseMachineLearning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.fit(x_credit_train,y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_credit = neural_network.predict(x_credit_test)\n",
    "predict_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_credit = accuracy_score(y_credit_test, predict_credit)\n",
    "accuracy_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test,predict_credit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_census.plk','rb') as d:\n",
    "    x_census_test, y_census_test, x_census_train, y_census_train = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9769, 108), (9769,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_test.shape, y_census_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22792, 108), (22792,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_train.shape, y_census_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_census = MLPClassifier(activation='relu',solver='adam',max_iter=1500,verbose=True,tol=0.0000100,hidden_layer_sizes=(55,55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41107155\n",
      "Iteration 2, loss = 0.32946079\n",
      "Iteration 3, loss = 0.31770686\n",
      "Iteration 4, loss = 0.30966812\n",
      "Iteration 5, loss = 0.30372745\n",
      "Iteration 6, loss = 0.29967474\n",
      "Iteration 7, loss = 0.29574653\n",
      "Iteration 8, loss = 0.29369493\n",
      "Iteration 9, loss = 0.29051333\n",
      "Iteration 10, loss = 0.28821008\n",
      "Iteration 11, loss = 0.28604319\n",
      "Iteration 12, loss = 0.28361219\n",
      "Iteration 13, loss = 0.28142764\n",
      "Iteration 14, loss = 0.28025207\n",
      "Iteration 15, loss = 0.27866338\n",
      "Iteration 16, loss = 0.27591193\n",
      "Iteration 17, loss = 0.27455229\n",
      "Iteration 18, loss = 0.27274087\n",
      "Iteration 19, loss = 0.27055950\n",
      "Iteration 20, loss = 0.26918095\n",
      "Iteration 21, loss = 0.26685771\n",
      "Iteration 22, loss = 0.26587024\n",
      "Iteration 23, loss = 0.26312077\n",
      "Iteration 24, loss = 0.26386637\n",
      "Iteration 25, loss = 0.26168377\n",
      "Iteration 26, loss = 0.26018599\n",
      "Iteration 27, loss = 0.25856522\n",
      "Iteration 28, loss = 0.25680724\n",
      "Iteration 29, loss = 0.25596815\n",
      "Iteration 30, loss = 0.25435441\n",
      "Iteration 31, loss = 0.25258961\n",
      "Iteration 32, loss = 0.25147154\n",
      "Iteration 33, loss = 0.25079928\n",
      "Iteration 34, loss = 0.24923272\n",
      "Iteration 35, loss = 0.24774816\n",
      "Iteration 36, loss = 0.24703137\n",
      "Iteration 37, loss = 0.24533506\n",
      "Iteration 38, loss = 0.24337252\n",
      "Iteration 39, loss = 0.24295913\n",
      "Iteration 40, loss = 0.24193304\n",
      "Iteration 41, loss = 0.24039471\n",
      "Iteration 42, loss = 0.23895699\n",
      "Iteration 43, loss = 0.23754079\n",
      "Iteration 44, loss = 0.23732227\n",
      "Iteration 45, loss = 0.23665505\n",
      "Iteration 46, loss = 0.23450524\n",
      "Iteration 47, loss = 0.23360128\n",
      "Iteration 48, loss = 0.23184803\n",
      "Iteration 49, loss = 0.23339987\n",
      "Iteration 50, loss = 0.23050756\n",
      "Iteration 51, loss = 0.23021421\n",
      "Iteration 52, loss = 0.22847339\n",
      "Iteration 53, loss = 0.22788869\n",
      "Iteration 54, loss = 0.22663357\n",
      "Iteration 55, loss = 0.22593823\n",
      "Iteration 56, loss = 0.22558303\n",
      "Iteration 57, loss = 0.22617240\n",
      "Iteration 58, loss = 0.22419972\n",
      "Iteration 59, loss = 0.22272112\n",
      "Iteration 60, loss = 0.22123926\n",
      "Iteration 61, loss = 0.22099255\n",
      "Iteration 62, loss = 0.22055038\n",
      "Iteration 63, loss = 0.21890025\n",
      "Iteration 64, loss = 0.21840688\n",
      "Iteration 65, loss = 0.21707601\n",
      "Iteration 66, loss = 0.21710029\n",
      "Iteration 67, loss = 0.21538426\n",
      "Iteration 68, loss = 0.21536818\n",
      "Iteration 69, loss = 0.21457152\n",
      "Iteration 70, loss = 0.21337244\n",
      "Iteration 71, loss = 0.21255007\n",
      "Iteration 72, loss = 0.21207613\n",
      "Iteration 73, loss = 0.21087870\n",
      "Iteration 74, loss = 0.21009438\n",
      "Iteration 75, loss = 0.21047454\n",
      "Iteration 76, loss = 0.20794982\n",
      "Iteration 77, loss = 0.20745552\n",
      "Iteration 78, loss = 0.20721848\n",
      "Iteration 79, loss = 0.20642027\n",
      "Iteration 80, loss = 0.20590726\n",
      "Iteration 81, loss = 0.20466574\n",
      "Iteration 82, loss = 0.20560599\n",
      "Iteration 83, loss = 0.20385717\n",
      "Iteration 84, loss = 0.20412819\n",
      "Iteration 85, loss = 0.20250656\n",
      "Iteration 86, loss = 0.20210966\n",
      "Iteration 87, loss = 0.20072911\n",
      "Iteration 88, loss = 0.20033173\n",
      "Iteration 89, loss = 0.19938660\n",
      "Iteration 90, loss = 0.19877840\n",
      "Iteration 91, loss = 0.19823648\n",
      "Iteration 92, loss = 0.19733455\n",
      "Iteration 93, loss = 0.19833194\n",
      "Iteration 94, loss = 0.19802927\n",
      "Iteration 95, loss = 0.19574343\n",
      "Iteration 96, loss = 0.19581556\n",
      "Iteration 97, loss = 0.19501279\n",
      "Iteration 98, loss = 0.19499226\n",
      "Iteration 99, loss = 0.19470368\n",
      "Iteration 100, loss = 0.19426203\n",
      "Iteration 101, loss = 0.19285766\n",
      "Iteration 102, loss = 0.19199464\n",
      "Iteration 103, loss = 0.19162625\n",
      "Iteration 104, loss = 0.19161434\n",
      "Iteration 105, loss = 0.19051709\n",
      "Iteration 106, loss = 0.18998749\n",
      "Iteration 107, loss = 0.19020863\n",
      "Iteration 108, loss = 0.18854620\n",
      "Iteration 109, loss = 0.18847297\n",
      "Iteration 110, loss = 0.18830154\n",
      "Iteration 111, loss = 0.18937298\n",
      "Iteration 112, loss = 0.18667657\n",
      "Iteration 113, loss = 0.18697099\n",
      "Iteration 114, loss = 0.18624566\n",
      "Iteration 115, loss = 0.18562444\n",
      "Iteration 116, loss = 0.18479763\n",
      "Iteration 117, loss = 0.18381112\n",
      "Iteration 118, loss = 0.18339978\n",
      "Iteration 119, loss = 0.18308752\n",
      "Iteration 120, loss = 0.18384549\n",
      "Iteration 121, loss = 0.18307889\n",
      "Iteration 122, loss = 0.18144466\n",
      "Iteration 123, loss = 0.18289631\n",
      "Iteration 124, loss = 0.18122172\n",
      "Iteration 125, loss = 0.18173365\n",
      "Iteration 126, loss = 0.18244574\n",
      "Iteration 127, loss = 0.18171131\n",
      "Iteration 128, loss = 0.18044434\n",
      "Iteration 129, loss = 0.17917475\n",
      "Iteration 130, loss = 0.17902922\n",
      "Iteration 131, loss = 0.17886913\n",
      "Iteration 132, loss = 0.17799573\n",
      "Iteration 133, loss = 0.17682199\n",
      "Iteration 134, loss = 0.17783640\n",
      "Iteration 135, loss = 0.17809077\n",
      "Iteration 136, loss = 0.17634610\n",
      "Iteration 137, loss = 0.17892010\n",
      "Iteration 138, loss = 0.17497096\n",
      "Iteration 139, loss = 0.17490494\n",
      "Iteration 140, loss = 0.17547626\n",
      "Iteration 141, loss = 0.17571750\n",
      "Iteration 142, loss = 0.17372488\n",
      "Iteration 143, loss = 0.17384102\n",
      "Iteration 144, loss = 0.17564544\n",
      "Iteration 145, loss = 0.17363581\n",
      "Iteration 146, loss = 0.17124933\n",
      "Iteration 147, loss = 0.17253072\n",
      "Iteration 148, loss = 0.17238313\n",
      "Iteration 149, loss = 0.17055159\n",
      "Iteration 150, loss = 0.17070849\n",
      "Iteration 151, loss = 0.17042895\n",
      "Iteration 152, loss = 0.16984732\n",
      "Iteration 153, loss = 0.16990847\n",
      "Iteration 154, loss = 0.16851446\n",
      "Iteration 155, loss = 0.16932446\n",
      "Iteration 156, loss = 0.16922553\n",
      "Iteration 157, loss = 0.16799119\n",
      "Iteration 158, loss = 0.16954401\n",
      "Iteration 159, loss = 0.16755436\n",
      "Iteration 160, loss = 0.16684820\n",
      "Iteration 161, loss = 0.16773377\n",
      "Iteration 162, loss = 0.16703858\n",
      "Iteration 163, loss = 0.16625527\n",
      "Iteration 164, loss = 0.16962837\n",
      "Iteration 165, loss = 0.16631257\n",
      "Iteration 166, loss = 0.16698835\n",
      "Iteration 167, loss = 0.16584363\n",
      "Iteration 168, loss = 0.16601377\n",
      "Iteration 169, loss = 0.16620899\n",
      "Iteration 170, loss = 0.16336747\n",
      "Iteration 171, loss = 0.16438078\n",
      "Iteration 172, loss = 0.16427391\n",
      "Iteration 173, loss = 0.16211602\n",
      "Iteration 174, loss = 0.16256216\n",
      "Iteration 175, loss = 0.16263475\n",
      "Iteration 176, loss = 0.16446925\n",
      "Iteration 177, loss = 0.16400649\n",
      "Iteration 178, loss = 0.16208588\n",
      "Iteration 179, loss = 0.16236488\n",
      "Iteration 180, loss = 0.16071368\n",
      "Iteration 181, loss = 0.16254865\n",
      "Iteration 182, loss = 0.16025664\n",
      "Iteration 183, loss = 0.16016954\n",
      "Iteration 184, loss = 0.16009428\n",
      "Iteration 185, loss = 0.15952949\n",
      "Iteration 186, loss = 0.15895550\n",
      "Iteration 187, loss = 0.15804430\n",
      "Iteration 188, loss = 0.15985913\n",
      "Iteration 189, loss = 0.15868268\n",
      "Iteration 190, loss = 0.15916528\n",
      "Iteration 191, loss = 0.15822058\n",
      "Iteration 192, loss = 0.15709270\n",
      "Iteration 193, loss = 0.15794474\n",
      "Iteration 194, loss = 0.15889381\n",
      "Iteration 195, loss = 0.15690884\n",
      "Iteration 196, loss = 0.15546669\n",
      "Iteration 197, loss = 0.15691429\n",
      "Iteration 198, loss = 0.15657100\n",
      "Iteration 199, loss = 0.15647807\n",
      "Iteration 200, loss = 0.15399630\n",
      "Iteration 201, loss = 0.15559086\n",
      "Iteration 202, loss = 0.15607542\n",
      "Iteration 203, loss = 0.15433091\n",
      "Iteration 204, loss = 0.15412358\n",
      "Iteration 205, loss = 0.15470441\n",
      "Iteration 206, loss = 0.15357892\n",
      "Iteration 207, loss = 0.15347237\n",
      "Iteration 208, loss = 0.15297662\n",
      "Iteration 209, loss = 0.15283635\n",
      "Iteration 210, loss = 0.15243479\n",
      "Iteration 211, loss = 0.15237478\n",
      "Iteration 212, loss = 0.15241168\n",
      "Iteration 213, loss = 0.15206431\n",
      "Iteration 214, loss = 0.15361110\n",
      "Iteration 215, loss = 0.15215830\n",
      "Iteration 216, loss = 0.15395092\n",
      "Iteration 217, loss = 0.15135903\n",
      "Iteration 218, loss = 0.15104367\n",
      "Iteration 219, loss = 0.15010341\n",
      "Iteration 220, loss = 0.14954722\n",
      "Iteration 221, loss = 0.14893239\n",
      "Iteration 222, loss = 0.14977826\n",
      "Iteration 223, loss = 0.15097499\n",
      "Iteration 224, loss = 0.15067141\n",
      "Iteration 225, loss = 0.14929362\n",
      "Iteration 226, loss = 0.14728415\n",
      "Iteration 227, loss = 0.14775741\n",
      "Iteration 228, loss = 0.14862315\n",
      "Iteration 229, loss = 0.14878328\n",
      "Iteration 230, loss = 0.14811065\n",
      "Iteration 231, loss = 0.14803154\n",
      "Iteration 232, loss = 0.14716596\n",
      "Iteration 233, loss = 0.14478210\n",
      "Iteration 234, loss = 0.14755364\n",
      "Iteration 235, loss = 0.14747561\n",
      "Iteration 236, loss = 0.14568076\n",
      "Iteration 237, loss = 0.14634801\n",
      "Iteration 238, loss = 0.14890445\n",
      "Iteration 239, loss = 0.14551373\n",
      "Iteration 240, loss = 0.14543328\n",
      "Iteration 241, loss = 0.14432506\n",
      "Iteration 242, loss = 0.14604879\n",
      "Iteration 243, loss = 0.14484590\n",
      "Iteration 244, loss = 0.14293488\n",
      "Iteration 245, loss = 0.14667616\n",
      "Iteration 246, loss = 0.14482156\n",
      "Iteration 247, loss = 0.14353725\n",
      "Iteration 248, loss = 0.14418772\n",
      "Iteration 249, loss = 0.14279744\n",
      "Iteration 250, loss = 0.14400155\n",
      "Iteration 251, loss = 0.14513470\n",
      "Iteration 252, loss = 0.14311629\n",
      "Iteration 253, loss = 0.14196598\n",
      "Iteration 254, loss = 0.14252125\n",
      "Iteration 255, loss = 0.14265466\n",
      "Iteration 256, loss = 0.14225117\n",
      "Iteration 257, loss = 0.14070085\n",
      "Iteration 258, loss = 0.14042521\n",
      "Iteration 259, loss = 0.14094742\n",
      "Iteration 260, loss = 0.13981406\n",
      "Iteration 261, loss = 0.14120580\n",
      "Iteration 262, loss = 0.14243255\n",
      "Iteration 263, loss = 0.14192731\n",
      "Iteration 264, loss = 0.13964048\n",
      "Iteration 265, loss = 0.14079193\n",
      "Iteration 266, loss = 0.14012619\n",
      "Iteration 267, loss = 0.14116328\n",
      "Iteration 268, loss = 0.13951337\n",
      "Iteration 269, loss = 0.14082206\n",
      "Iteration 270, loss = 0.13924821\n",
      "Iteration 271, loss = 0.13887200\n",
      "Iteration 272, loss = 0.14049014\n",
      "Iteration 273, loss = 0.13954298\n",
      "Iteration 274, loss = 0.13850281\n",
      "Iteration 275, loss = 0.14003960\n",
      "Iteration 276, loss = 0.13812264\n",
      "Iteration 277, loss = 0.14251789\n",
      "Iteration 278, loss = 0.14037622\n",
      "Iteration 279, loss = 0.13677293\n",
      "Iteration 280, loss = 0.13696064\n",
      "Iteration 281, loss = 0.13694518\n",
      "Iteration 282, loss = 0.13578861\n",
      "Iteration 283, loss = 0.13924442\n",
      "Iteration 284, loss = 0.13635107\n",
      "Iteration 285, loss = 0.13647383\n",
      "Iteration 286, loss = 0.13786733\n",
      "Iteration 287, loss = 0.13534372\n",
      "Iteration 288, loss = 0.13638103\n",
      "Iteration 289, loss = 0.13509227\n",
      "Iteration 290, loss = 0.13665414\n",
      "Iteration 291, loss = 0.13436793\n",
      "Iteration 292, loss = 0.13591749\n",
      "Iteration 293, loss = 0.13466179\n",
      "Iteration 294, loss = 0.13645969\n",
      "Iteration 295, loss = 0.13479276\n",
      "Iteration 296, loss = 0.13356746\n",
      "Iteration 297, loss = 0.13537826\n",
      "Iteration 298, loss = 0.13560882\n",
      "Iteration 299, loss = 0.13321695\n",
      "Iteration 300, loss = 0.13465407\n",
      "Iteration 301, loss = 0.13707051\n",
      "Iteration 302, loss = 0.13517262\n",
      "Iteration 303, loss = 0.13255831\n",
      "Iteration 304, loss = 0.13236032\n",
      "Iteration 305, loss = 0.13411068\n",
      "Iteration 306, loss = 0.13108727\n",
      "Iteration 307, loss = 0.13321156\n",
      "Iteration 308, loss = 0.13189686\n",
      "Iteration 309, loss = 0.13124244\n",
      "Iteration 310, loss = 0.13142716\n",
      "Iteration 311, loss = 0.13346396\n",
      "Iteration 312, loss = 0.13093787\n",
      "Iteration 313, loss = 0.13123837\n",
      "Iteration 314, loss = 0.13208134\n",
      "Iteration 315, loss = 0.13100818\n",
      "Iteration 316, loss = 0.13302995\n",
      "Iteration 317, loss = 0.12987470\n",
      "Iteration 318, loss = 0.13073825\n",
      "Iteration 319, loss = 0.13083912\n",
      "Iteration 320, loss = 0.13134589\n",
      "Iteration 321, loss = 0.13007920\n",
      "Iteration 322, loss = 0.13423052\n",
      "Iteration 323, loss = 0.13253149\n",
      "Iteration 324, loss = 0.13011570\n",
      "Iteration 325, loss = 0.12950412\n",
      "Iteration 326, loss = 0.12983505\n",
      "Iteration 327, loss = 0.12846225\n",
      "Iteration 328, loss = 0.12915196\n",
      "Iteration 329, loss = 0.13136609\n",
      "Iteration 330, loss = 0.12914221\n",
      "Iteration 331, loss = 0.12827403\n",
      "Iteration 332, loss = 0.12810091\n",
      "Iteration 333, loss = 0.12888663\n",
      "Iteration 334, loss = 0.12780165\n",
      "Iteration 335, loss = 0.12635520\n",
      "Iteration 336, loss = 0.12879365\n",
      "Iteration 337, loss = 0.12914534\n",
      "Iteration 338, loss = 0.12746485\n",
      "Iteration 339, loss = 0.12728609\n",
      "Iteration 340, loss = 0.12857312\n",
      "Iteration 341, loss = 0.12638136\n",
      "Iteration 342, loss = 0.12762337\n",
      "Iteration 343, loss = 0.12698412\n",
      "Iteration 344, loss = 0.12636812\n",
      "Iteration 345, loss = 0.12698206\n",
      "Iteration 346, loss = 0.12627822\n",
      "Iteration 347, loss = 0.12624316\n",
      "Iteration 348, loss = 0.12526325\n",
      "Iteration 349, loss = 0.12571677\n",
      "Iteration 350, loss = 0.12510812\n",
      "Iteration 351, loss = 0.12521926\n",
      "Iteration 352, loss = 0.12574958\n",
      "Iteration 353, loss = 0.12805405\n",
      "Iteration 354, loss = 0.12631495\n",
      "Iteration 355, loss = 0.12430736\n",
      "Iteration 356, loss = 0.12451888\n",
      "Iteration 357, loss = 0.12408027\n",
      "Iteration 358, loss = 0.12506363\n",
      "Iteration 359, loss = 0.12486530\n",
      "Iteration 360, loss = 0.12739097\n",
      "Iteration 361, loss = 0.12607964\n",
      "Iteration 362, loss = 0.12593514\n",
      "Iteration 363, loss = 0.12540605\n",
      "Iteration 364, loss = 0.12283765\n",
      "Iteration 365, loss = 0.12449484\n",
      "Iteration 366, loss = 0.12598413\n",
      "Iteration 367, loss = 0.12387304\n",
      "Iteration 368, loss = 0.12363532\n",
      "Iteration 369, loss = 0.12405825\n",
      "Iteration 370, loss = 0.12287333\n",
      "Iteration 371, loss = 0.12283816\n",
      "Iteration 372, loss = 0.12440570\n",
      "Iteration 373, loss = 0.12508082\n",
      "Iteration 374, loss = 0.12356159\n",
      "Iteration 375, loss = 0.12317642\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_census.fit(x_census_train,y_census_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_census = neural_network_census.predict(x_census_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8204524516327157"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_census = accuracy_score(y_census_test,predict_census)\n",
    "accuracy_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.89      0.88      7407\n",
      "        >50K       0.64      0.59      0.61      2362\n",
      "\n",
      "    accuracy                           0.82      9769\n",
      "   macro avg       0.76      0.74      0.75      9769\n",
      "weighted avg       0.82      0.82      0.82      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test,predict_census))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
